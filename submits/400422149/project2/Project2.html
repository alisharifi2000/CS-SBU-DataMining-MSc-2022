<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-05-19 Thu 20:36 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Project2</title>
<meta name="author" content="Mohammadb Ghorbani" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
</style>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Project2</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgb57da79">1. Mobile Data</a>
<ul>
<li><a href="#org3f914a9">1.1. imports</a></li>
<li><a href="#org083bd80">1.2. read data</a>
<ul>
<li><a href="#org52f19f1">1.2.1. change price data format</a></li>
</ul>
</li>
<li><a href="#orgc8ea97a">1.3. Forward Selection</a></li>
<li><a href="#orga735f4a">1.4. PCA &amp; Train Logistic Model</a></li>
<li><a href="#orge4ae29c">1.5. SVM</a>
<ul>
<li><a href="#org3b54762">1.5.1. Train Model</a></li>
<li><a href="#orgb140de3">1.5.2. Results</a></li>
<li><a href="#org4e0dc97">1.5.3. Tuning Hyperparameters</a></li>
</ul>
</li>
<li><a href="#orgf719b57">1.6. Tree</a>
<ul>
<li><a href="#org231a245">1.6.1. Tuning Hyperparameters</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgd668e8e">2. Heart</a>
<ul>
<li><a href="#org51fc4c8">2.1. imports</a></li>
<li><a href="#orgb6ce6f4">2.2. read data</a></li>
<li><a href="#orge0bc92e">2.3. Gaussian Naive Bayes</a></li>
</ul>
</li>
<li><a href="#org0c50c56">3. Questions</a>
<ul>
<li><a href="#org42216a1">3.1. Task 1</a>
<ul>
<li><a href="#orga26e4a4">3.1.1. Q 5</a></li>
<li><a href="#orgd6379b9">3.1.2. Q 9 part B</a></li>
<li><a href="#orged8eb15">3.1.3. Q 9 , part C</a></li>
<li><a href="#org48d49cb">3.1.4. Q 11</a></li>
<li><a href="#org173ca26">3.1.5. Q 14</a></li>
<li><a href="#orge80284d">3.1.6. Q 15</a></li>
<li><a href="#org47fcaee">3.1.7. Q 16</a></li>
</ul>
</li>
<li><a href="#org31dab9b">3.2. Task 2</a>
<ul>
<li><a href="#orgd9a4797">3.2.1. Q 1</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>


<div id="outline-container-orgb57da79" class="outline-2">
<h2 id="orgb57da79"><span class="section-number-2">1.</span> Mobile Data</h2>
<div class="outline-text-2" id="text-1">
</div>

<div id="outline-container-org3f914a9" class="outline-3">
<h3 id="org3f914a9"><span class="section-number-3">1.1.</span> imports</h3>
<div class="outline-text-3" id="text-1-1">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span style="color: #51afef;">import</span> seaborn <span style="color: #51afef;">as</span> sns
<span style="color: #51afef;">from</span> matplotlib.colors <span style="color: #51afef;">import</span> ListedColormap
<span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span style="color: #51afef;">from</span> sklearn.linear_model <span style="color: #51afef;">import</span> LogisticRegression
<span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> roc_auc_score
<span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> classification_report
<span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> PCA
<span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> svm
<span style="color: #51afef;">from</span> sklearn.tree <span style="color: #51afef;">import</span> DecisionTreeClassifier
<span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> metrics
<span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span style="color: #51afef;">from</span> mpl_toolkits.mplot3d <span style="color: #51afef;">import</span> Axes3D

</pre>
</div>
</div>
</div>

<div id="outline-container-org083bd80" class="outline-3">
<h3 id="org083bd80"><span class="section-number-3">1.2.</span> read data</h3>
<div class="outline-text-3" id="text-1-2">
<div class="org-src-container">
<pre class="src src-python">
<span style="color: #dcaeea;">raw_train_data</span> = pd.read_csv(<span style="color: #98be65;">'train.csv'</span>)
<span style="color: #dcaeea;">raw_test_data</span> = pd.read_csv(<span style="color: #98be65;">'test.csv'</span>)

</pre>
</div>

<div class="org-src-container">
<pre class="src src-python">
<span style="color: #c678dd;">print</span>(raw_train_data.head())
<span style="color: #c678dd;">print</span>(raw_train_data.isnull().<span style="color: #c678dd;">sum</span>())
</pre>
</div>

<pre class="example" id="orgc1238a5">
   battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  m_dep  mobile_wt  n_cores  pc  px_height  px_width   ram  sc_h  sc_w  talk_time  three_g  touch_screen  wifi  price_range
0            842     0          2.2         0   1       0           7    0.6        188        2   2         20       756  2549     9     7         19        0             0     1            1
1           1021     1          0.5         1   0       1          53    0.7        136        3   6        905      1988  2631    17     3          7        1             1     0            2
2            563     1          0.5         1   2       1          41    0.9        145        5   6       1263      1716  2603    11     2          9        1             1     0            2
3            615     1          2.5         0   0       0          10    0.8        131        6   9       1216      1786  2769    16     8         11        1             0     0            2
4           1821     1          1.2         0  13       1          44    0.6        141        2  14       1208      1212  1411     8     2         15        1             1     0            1
battery_power    0
blue             0
clock_speed      0
dual_sim         0
fc               0
four_g           0
int_memory       0
m_dep            0
mobile_wt        0
n_cores          0
pc               0
px_height        0
px_width         0
ram              0
sc_h             0
sc_w             0
talk_time        0
three_g          0
touch_screen     0
wifi             0
price_range      0
dtype: int64
</pre>
</div>

<div id="outline-container-org52f19f1" class="outline-4">
<h4 id="org52f19f1"><span class="section-number-4">1.2.1.</span> change price data format</h4>
<div class="outline-text-4" id="text-1-2-1">
<div class="org-src-container">
<pre class="src src-python">
<span style="color: #51afef;">def</span> <span style="color: #c678dd;">concat_price_to_two_classes</span>(price_range):

   <span style="color: #51afef;">if</span> price_range == <span style="color: #da8548; font-weight: bold;">3</span> <span style="color: #51afef;">or</span> price_range == <span style="color: #da8548; font-weight: bold;">2</span>:
      <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">1</span>
   <span style="color: #51afef;">else</span>:
      <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">0</span>


raw_train_data.price_range = raw_train_data.price_range.<span style="color: #c678dd;">apply</span>(concat_price_to_two_classes)


</pre>
</div>
</div>
</div>
</div>


<div id="outline-container-orgc8ea97a" class="outline-3">
<h3 id="orgc8ea97a"><span class="section-number-3">1.3.</span> Forward Selection</h3>
<div class="outline-text-3" id="text-1-3">
<div class="org-src-container">
<pre class="src src-python">
<span style="color: #dcaeea;">y</span> = raw_train_data.price_range
<span style="color: #dcaeea;">train_data</span> = raw_train_data.drop([<span style="color: #98be65;">'price_range'</span>],axis=<span style="color: #da8548; font-weight: bold;">1</span>)

</pre>
</div>

<div class="org-src-container">
<pre class="src src-python">
<span style="color: #dcaeea;">feature_ind</span> = []
<span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #c678dd;">len</span>(train_data.columns)) :
    <span style="color: #dcaeea;">selected_features</span> = []
    <span style="color: #51afef;">for</span> j, feature <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(train_data.columns):
        <span style="color: #51afef;">if</span> j == i:
            <span style="color: #51afef;">break</span>
        selected_features.append(feature)

    <span style="color: #51afef;">if</span> <span style="color: #c678dd;">len</span>(selected_features) &gt; <span style="color: #da8548; font-weight: bold;">0</span>:
        feature_ind.append(selected_features)

</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #dcaeea;">perv_res</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span style="color: #dcaeea;">res</span> = {}
<span style="color: #dcaeea;">junk_features</span> = []
<span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #c678dd;">len</span>(feature_ind)):

    <span style="color: #51afef;">if</span> i == <span style="color: #da8548; font-weight: bold;">0</span>:
        x = train_data[feature_ind[i]]
        <span style="color: #dcaeea;">x_train</span>, <span style="color: #dcaeea;">x_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(x, y, test_size=<span style="color: #da8548; font-weight: bold;">0.2</span>)
        model = LogisticRegression(solver=<span style="color: #98be65;">'liblinear'</span>, C=<span style="color: #da8548; font-weight: bold;">10.0</span>)
        model.fit(x_train, y_train)
        result = roc_auc_score(y_test, model.predict(x_test))
        res[<span style="color: #c678dd;">str</span>(feature_ind[i])] = result
    <span style="color: #51afef;">else</span>:
        clean = [z <span style="color: #51afef;">for</span> z <span style="color: #51afef;">in</span> feature_ind[i] <span style="color: #51afef;">if</span> z <span style="color: #51afef;">not</span> <span style="color: #51afef;">in</span> junk_features]
        x = train_data[clean]
        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span style="color: #da8548; font-weight: bold;">0.2</span>)

        model = LogisticRegression(solver=<span style="color: #98be65;">'liblinear'</span>, C=<span style="color: #da8548; font-weight: bold;">10.0</span>)
        model.fit(x_train, y_train)
        result = roc_auc_score(y_test, model.predict(x_test))
        res[<span style="color: #c678dd;">str</span>(clean)] = result
    <span style="color: #51afef;">if</span> (perv_res &gt; result):

        selected_features = feature_ind[i]
        junk = selected_features[-<span style="color: #da8548; font-weight: bold;">1</span>]

        junk_features.append(junk)

    perv_res = result


<span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">len</span>(clean))



</pre>
</div>

<pre class="example">
10
</pre>


<div class="org-src-container">
<pre class="src src-python">
<span style="color: #dcaeea;">x</span> = train_data[clean]
<span style="color: #dcaeea;">x_train</span>, <span style="color: #dcaeea;">x_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(x, y, test_size=<span style="color: #da8548; font-weight: bold;">0.2</span>)
model = LogisticRegression(solver=<span style="color: #98be65;">'liblinear'</span>, C=<span style="color: #da8548; font-weight: bold;">10.0</span>)
model.fit(x_train, y_train)


<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"clean features are : "</span> + <span style="color: #c678dd;">str</span>(clean))
<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"junk_features are  :"</span> + <span style="color: #c678dd;">str</span>(junk_features))
<span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">max</span>(res.values()))


<span style="color: #c678dd;">print</span>(classification_report(y_test, model.predict(x_test)))

</pre>
</div>

<pre class="example" id="org0afe90a">
clean features are : ['battery_power', 'dual_sim', 'four_g', 'm_dep', 'mobile_wt', 'px_height', 'ram', 'sc_w', 'three_g', 'touch_screen']
junk_features are  :['blue', 'clock_speed', 'fc', 'int_memory', 'n_cores', 'pc', 'px_width', 'sc_h', 'talk_time', 'touch_screen']
0.9698727122090833
              precision    recall  f1-score   support

           0       0.97      0.96      0.96       209
           1       0.96      0.96      0.96       191

    accuracy                           0.96       400
   macro avg       0.96      0.96      0.96       400
weighted avg       0.96      0.96      0.96       400
</pre>
</div>
</div>


<div id="outline-container-orga735f4a" class="outline-3">
<h3 id="orga735f4a"><span class="section-number-3">1.4.</span> PCA &amp; Train Logistic Model</h3>
<div class="outline-text-3" id="text-1-4">
<div class="org-src-container">
<pre class="src src-python">

<span style="color: #dcaeea;">pca</span> = PCA(n_components=<span style="color: #c678dd;">len</span>(clean), )

x_train_pca = pca.fit_transform(train_data[clean])

<span style="color: #dcaeea;">x_train</span>, <span style="color: #dcaeea;">x_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(x_train_pca, y, test_size=<span style="color: #da8548; font-weight: bold;">0.2</span>)
model = LogisticRegression(solver=<span style="color: #98be65;">'liblinear'</span>, C=<span style="color: #da8548; font-weight: bold;">10.0</span>)
model.fit(x_train, y_train)


<span style="color: #c678dd;">print</span>(classification_report(y_test, model.predict(x_test)))



</pre>
</div>

<pre class="example">
              precision    recall  f1-score   support

           0       0.95      0.95      0.95       199
           1       0.95      0.96      0.95       201

    accuracy                           0.95       400
   macro avg       0.95      0.95      0.95       400
weighted avg       0.95      0.95      0.95       400
</pre>
</div>
</div>




<div id="outline-container-orge4ae29c" class="outline-3">
<h3 id="orge4ae29c"><span class="section-number-3">1.5.</span> SVM</h3>
<div class="outline-text-3" id="text-1-5">
</div>
<div id="outline-container-org3b54762" class="outline-4">
<h4 id="org3b54762"><span class="section-number-4">1.5.1.</span> Train Model</h4>
<div class="outline-text-4" id="text-1-5-1">
<div class="org-src-container">
<pre class="src src-python">

<span style="color: #dcaeea;">clf</span> = svm.SVC(kernel=<span style="color: #98be65;">'linear'</span>)

clf.fit(x_train, y_train)

y_pred = clf.predict(x_test)




</pre>
</div>
</div>
</div>

<div id="outline-container-orgb140de3" class="outline-4">
<h4 id="orgb140de3"><span class="section-number-4">1.5.2.</span> Results</h4>
<div class="outline-text-4" id="text-1-5-2">
<div class="org-src-container">
<pre class="src src-python">

<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Accuracy:"</span>,metrics.accuracy_score(y_test, y_pred))
<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Precision:"</span>,metrics.precision_score(y_test, y_pred))
<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Recall:"</span>,metrics.recall_score(y_test, y_pred))



</pre>
</div>

<pre class="example">
Accuracy: 0.9575
Precision: 0.9554455445544554
Recall: 0.9601990049751243
</pre>
</div>
</div>

<div id="outline-container-org4e0dc97" class="outline-4">
<h4 id="org4e0dc97"><span class="section-number-4">1.5.3.</span> Tuning Hyperparameters</h4>
<div class="outline-text-4" id="text-1-5-3">
<div class="org-src-container">
<pre class="src src-python">
<span style="color: #dcaeea;">svm_res</span> = {}

<span style="color: #dcaeea;">kernels</span> = [<span style="color: #98be65;">"linear"</span>, <span style="color: #98be65;">"poly"</span>, <span style="color: #98be65;">"rbf"</span>, <span style="color: #98be65;">"sigmoid"</span>, <span style="color: #98be65;">"linear"</span>]
<span style="color: #dcaeea;">c</span> = [<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">1.0</span>, <span style="color: #da8548; font-weight: bold;">10.0</span>, <span style="color: #da8548; font-weight: bold;">100.0</span>, <span style="color: #da8548; font-weight: bold;">0.001</span>]


<span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">5</span>):

    <span style="color: #dcaeea;">k</span> = <span style="color: #c678dd;">str</span>(kernels[i])
    <span style="color: #dcaeea;">f</span> = <span style="color: #c678dd;">float</span>(c[i])

    <span style="color: #dcaeea;">config</span> = <span style="color: #c678dd;">str</span>(kernels[i]) + <span style="color: #98be65;">" : "</span> +<span style="color: #c678dd;">str</span>(f) + <span style="color: #98be65;">" : "</span> + <span style="color: #98be65;">"scale"</span>
    <span style="color: #dcaeea;">clf</span> = svm.SVC(kernel=k,C=f , gamma=<span style="color: #98be65;">'scale'</span>)
    clf.fit(x_train, y_train)
    y_pred = clf.predict(x_test)
    acc = metrics.accuracy_score(y_test, y_pred)
    pre = metrics.precision_score(y_test, y_pred)
    recall = metrics.recall_score(y_test, y_pred)
    <span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">str</span>(config) + <span style="color: #98be65;">" : "</span> + <span style="color: #98be65;">"accuracy + precision + recall : "</span> + <span style="color: #c678dd;">str</span>((acc, pre, recall)))
    svm_res[<span style="color: #c678dd;">str</span>(config)] = (acc, pre, recall)








</pre>
</div>

<pre class="example">
linear : 0.1 : scale : accuracy + precision + recall : (0.9575, 0.9554455445544554, 0.9601990049751243)
poly : 1.0 : scale : accuracy + precision + recall : (0.955, 0.9597989949748744, 0.9502487562189055)
rbf : 10.0 : scale : accuracy + precision + recall : (0.9525, 0.9504950495049505, 0.9552238805970149)
sigmoid : 100.0 : scale : accuracy + precision + recall : (0.895, 0.9119170984455959, 0.8756218905472637)
linear : 0.001 : scale : accuracy + precision + recall : (0.955, 0.9552238805970149, 0.9552238805970149)
</pre>



<div class="org-src-container">
<pre class="src src-python">

<span style="color: #dcaeea;">power_bins</span> = [<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">800</span>, <span style="color: #da8548; font-weight: bold;">1000</span> , <span style="color: #da8548; font-weight: bold;">1200</span> , <span style="color: #da8548; font-weight: bold;">1998</span>]
<span style="color: #dcaeea;">test_data</span> = train_data
<span style="color: #dcaeea;">test_data</span>[<span style="color: #98be65;">"battery_power_b"</span>] = pd.cut(train_data[<span style="color: #98be65;">'battery_power'</span>], bins=power_bins, labels=[<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">3</span>,<span style="color: #da8548; font-weight: bold;">4</span>])
test_data.drop([<span style="color: #98be65;">'battery_power'</span>], axis=<span style="color: #da8548; font-weight: bold;">1</span>, inplace=<span style="color: #a9a1e1;">True</span>)



</pre>
</div>

<pre class="example">
None
</pre>



<div class="org-src-container">
<pre class="src src-python">
<span style="color: #dcaeea;">x</span> = test_data
<span style="color: #dcaeea;">x_train</span>, <span style="color: #dcaeea;">x_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(x, y, test_size=<span style="color: #da8548; font-weight: bold;">0.2</span>)
clf = svm.SVC(kernel=<span style="color: #98be65;">'linear'</span>)

clf.fit(x_train, y_train)

y_pred = clf.predict(x_test)


<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Accuracy:"</span>,metrics.accuracy_score(y_test, y_pred))
<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Precision:"</span>,metrics.precision_score(y_test, y_pred))
<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Recall:"</span>,metrics.recall_score(y_test, y_pred))



</pre>
</div>

<pre class="example">
Accuracy: 0.98
Precision: 0.981042654028436
Recall: 0.981042654028436
</pre>




<div class="org-src-container">
<pre class="src src-python"><span style="color: #dcaeea;">test_data</span> = pd.get_dummies(test_data, prefix=[<span style="color: #98be65;">'dual_sim'</span>,<span style="color: #98be65;">'four_g'</span>,<span style="color: #98be65;">'three_g'</span>,<span style="color: #98be65;">'touch_screen'</span>,<span style="color: #98be65;">'wifi'</span>,<span style="color: #98be65;">'battery_power_b'</span>],
               columns=[<span style="color: #98be65;">'dual_sim'</span>,<span style="color: #98be65;">'four_g'</span>,<span style="color: #98be65;">'three_g'</span>,<span style="color: #98be65;">'touch_screen'</span>,<span style="color: #98be65;">'wifi'</span>,<span style="color: #98be65;">'battery_power_b'</span>])


x = test_data
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>)
clf = svm.SVC(kernel=<span style="color: #98be65;">'linear'</span>)

clf.fit(x_train, y_train)

y_pred = clf.predict(x_test)


<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Accuracy:"</span>,metrics.accuracy_score(y_test, y_pred))
<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Precision:"</span>,metrics.precision_score(y_test, y_pred))
<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Recall:"</span>,metrics.recall_score(y_test, y_pred))



</pre>
</div>

<pre class="example">
Accuracy: 0.975
Precision: 0.9705882352941176
Recall: 0.9801980198019802
</pre>


<div class="org-src-container">
<pre class="src src-python">

<span style="color: #dcaeea;">test_data</span>[<span style="color: #98be65;">'surface'</span>] = test_data[<span style="color: #98be65;">'px_height'</span>] * test_data[<span style="color: #98be65;">'px_width'</span>]
<span style="color: #dcaeea;">test_data</span>[<span style="color: #98be65;">'surface_log'</span>] = np.log(test_data[<span style="color: #98be65;">'surface'</span>])
test_data.drop([<span style="color: #98be65;">'surface_log'</span>],axis=<span style="color: #da8548; font-weight: bold;">1</span>,inplace=<span style="color: #a9a1e1;">True</span>)

</pre>
</div>

<pre class="example">
None
</pre>
</div>
</div>
</div>

<div id="outline-container-orgf719b57" class="outline-3">
<h3 id="orgf719b57"><span class="section-number-3">1.6.</span> Tree</h3>
<div class="outline-text-3" id="text-1-6">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #dcaeea;">x</span> = test_data
<span style="color: #dcaeea;">x_train</span>, <span style="color: #dcaeea;">x_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(x, y, test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>)

clf = DecisionTreeClassifier()
clf = clf.fit(x_train,y_train)
y_pred = clf.predict(x_test)

<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Accuracy:"</span>,metrics.accuracy_score(y_test, y_pred))

</pre>
</div>

<pre class="example">
Accuracy: 0.9283333333333333
</pre>
</div>

<div id="outline-container-org231a245" class="outline-4">
<h4 id="org231a245"><span class="section-number-4">1.6.1.</span> Tuning Hyperparameters</h4>
<div class="outline-text-4" id="text-1-6-1">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #dcaeea;">c</span> = [<span style="color: #98be65;">"entropy"</span>, <span style="color: #98be65;">"gini"</span>, <span style="color: #98be65;">"gini"</span>]
<span style="color: #dcaeea;">depth</span> = [<span style="color: #da8548; font-weight: bold;">10</span>,<span style="color: #da8548; font-weight: bold;">40</span>, <span style="color: #da8548; font-weight: bold;">90</span>]
<span style="color: #dcaeea;">s</span> = [<span style="color: #98be65;">"random"</span>, <span style="color: #98be65;">"best"</span>, <span style="color: #98be65;">"random"</span>]
<span style="color: #dcaeea;">msl</span> = [<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">3</span>,<span style="color: #da8548; font-weight: bold;">4</span>]

<span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">3</span>):
    <span style="color: #dcaeea;">config</span> = <span style="color: #98be65;">"criterion : "</span> + <span style="color: #c678dd;">str</span>(c[i]) + <span style="color: #98be65;">" depth : "</span> + <span style="color: #c678dd;">str</span>(depth[i]) + <span style="color: #98be65;">" splitter : "</span> + <span style="color: #c678dd;">str</span>(s[i]) + <span style="color: #98be65;">" min_samples_leaf : "</span> + <span style="color: #c678dd;">str</span>(msl[i])
    <span style="color: #dcaeea;">clf</span> = DecisionTreeClassifier(criterion=c[i], max_depth=depth[i], splitter=s[i], min_samples_leaf=msl[i])
    clf = clf.fit(x_train,y_train)
    y_pred = clf.predict(x_test)
    <span style="color: #c678dd;">print</span>(config)
    <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Accuracy:"</span>,metrics.accuracy_score(y_test, y_pred))



</pre>
</div>

<pre class="example">
criterion : entropy depth : 10 splitter : random min_samples_leaf : 2
Accuracy: 0.9466666666666667
criterion : gini depth : 40 splitter : best min_samples_leaf : 3
Accuracy: 0.9333333333333333
criterion : gini depth : 90 splitter : random min_samples_leaf : 4
Accuracy: 0.96
</pre>
</div>
</div>
</div>
</div>



<div id="outline-container-orgd668e8e" class="outline-2">
<h2 id="orgd668e8e"><span class="section-number-2">2.</span> Heart</h2>
<div class="outline-text-2" id="text-2">
</div>


<div id="outline-container-org51fc4c8" class="outline-3">
<h3 id="org51fc4c8"><span class="section-number-3">2.1.</span> imports</h3>
<div class="outline-text-3" id="text-2-1">
<div class="org-src-container">
<pre class="src src-python">
<span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span style="color: #51afef;">import</span> math
<span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> classification_report
<span style="color: #51afef;">from</span> sklearn.naive_bayes <span style="color: #51afef;">import</span> GaussianNB



</pre>
</div>
</div>
</div>

<div id="outline-container-orgb6ce6f4" class="outline-3">
<h3 id="orgb6ce6f4"><span class="section-number-3">2.2.</span> read data</h3>
<div class="outline-text-3" id="text-2-2">
<div class="org-src-container">
<pre class="src src-python">
<span style="color: #dcaeea;">raw_data</span> = pd.read_csv(<span style="color: #98be65;">"heart.csv"</span>)
<span style="color: #dcaeea;">train_data</span> = raw_data[[<span style="color: #98be65;">'chol'</span>, <span style="color: #98be65;">'trestbps'</span>, <span style="color: #98be65;">'thalach'</span>, <span style="color: #98be65;">'target'</span>]]
</pre>
</div>
</div>
</div>


<div id="outline-container-orge0bc92e" class="outline-3">
<h3 id="orge0bc92e"><span class="section-number-3">2.3.</span> Gaussian Naive Bayes</h3>
<div class="outline-text-3" id="text-2-3">
<div class="org-src-container">
<pre class="src src-python">
<span style="color: #dcaeea;">means</span> = train_data.groupby([<span style="color: #98be65;">"target"</span>]).mean()
<span style="color: #dcaeea;">var</span> = train_data.groupby([<span style="color: #98be65;">"target"</span>]).var()
<span style="color: #dcaeea;">prior</span> = (train_data.groupby(<span style="color: #98be65;">"target"</span>).count() / <span style="color: #c678dd;">len</span>(train_data)).iloc[:,<span style="color: #da8548; font-weight: bold;">1</span>]
<span style="color: #dcaeea;">classes</span> = [<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>]


</pre>
</div>

<div class="org-src-container">
<pre class="src src-python">
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">got idea from another code</span>


<span style="color: #51afef;">def</span> <span style="color: #c678dd;">Normal</span>(n, mu, var):

    <span style="color: #dcaeea;">sd</span> = np.sqrt(var)
    <span style="color: #dcaeea;">pdf</span> = (np.e ** (-<span style="color: #da8548; font-weight: bold;">0.5</span> * ((n - mu)/sd) ** <span style="color: #da8548; font-weight: bold;">2</span>)) / (sd * np.sqrt(<span style="color: #da8548; font-weight: bold;">2</span> * np.pi))

    <span style="color: #51afef;">return</span> pdf

<span style="color: #51afef;">def</span> <span style="color: #c678dd;">Predict</span>(X):
    <span style="color: #dcaeea;">Predictions</span> = []

    <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> X.<span style="color: #dcaeea;">index</span>:

        ClassLikelihood = []
        <span style="color: #dcaeea;">instance</span> = X.loc[i]

        <span style="color: #51afef;">for</span> cls <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">classes</span>:

            FeatureLikelihoods = []
            FeatureLikelihoods.append(np.log(prior[cls]))

            <span style="color: #51afef;">for</span> col <span style="color: #51afef;">in</span> x_train.<span style="color: #dcaeea;">columns</span>:

                data = instance[col]

                <span style="color: #dcaeea;">mean</span> = means[col].loc[cls]
                <span style="color: #dcaeea;">variance</span> = var[col].loc[cls]

                <span style="color: #dcaeea;">Likelihood</span> = Normal(data, mean, variance)

                <span style="color: #51afef;">if</span> Likelihood != <span style="color: #da8548; font-weight: bold;">0</span>:
                    <span style="color: #dcaeea;">Likelihood</span> = np.log(Likelihood)
                <span style="color: #51afef;">else</span>:
                    Likelihood = <span style="color: #da8548; font-weight: bold;">1</span>/<span style="color: #c678dd;">len</span>(train)

                FeatureLikelihoods.append(Likelihood)

            <span style="color: #dcaeea;">TotalLikelihood</span> = <span style="color: #c678dd;">sum</span>(FeatureLikelihoods)
            ClassLikelihood.append(TotalLikelihood)

        <span style="color: #dcaeea;">MaxIndex</span> = ClassLikelihood.index(<span style="color: #c678dd;">max</span>(ClassLikelihood))
        <span style="color: #dcaeea;">Prediction</span> = classes[MaxIndex]
        Predictions.append(Prediction)

    <span style="color: #51afef;">return</span> Predictions



</pre>
</div>

<div class="org-src-container">
<pre class="src src-python">

<span style="color: #dcaeea;">train_data</span> = raw_data[[<span style="color: #98be65;">'chol'</span>, <span style="color: #98be65;">'trestbps'</span>, <span style="color: #98be65;">'thalach'</span>]]
<span style="color: #dcaeea;">y</span> = raw_data.target
<span style="color: #dcaeea;">x_train</span>, <span style="color: #dcaeea;">x_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(train_data, y, test_size=<span style="color: #da8548; font-weight: bold;">0.2</span>)


</pre>
</div>

<div class="org-src-container">
<pre class="src src-python">
<span style="color: #dcaeea;">PredictTrain</span> = Predict(x_train)
<span style="color: #dcaeea;">PredictTest</span> = Predict(x_test)


</pre>
</div>

<div class="org-src-container">
<pre class="src src-python">
<span style="color: #51afef;">def</span> <span style="color: #c678dd;">Accuracy</span>(y, prediction):

    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Function to calculate accuracy</span>
    <span style="color: #dcaeea;">y</span> = <span style="color: #c678dd;">list</span>(y)
    <span style="color: #dcaeea;">prediction</span> = <span style="color: #c678dd;">list</span>(prediction)
    <span style="color: #dcaeea;">score</span> = <span style="color: #da8548; font-weight: bold;">0</span>

    <span style="color: #51afef;">for</span> i, j <span style="color: #51afef;">in</span> <span style="color: #c678dd;">zip</span>(y, prediction):
        <span style="color: #51afef;">if</span> i == j:
            score += <span style="color: #da8548; font-weight: bold;">1</span>

    <span style="color: #51afef;">return</span> score / <span style="color: #c678dd;">len</span>(y)



</pre>
</div>

<div class="org-src-container">
<pre class="src src-python">
<span style="color: #dcaeea;">acc_tr</span> = Accuracy(y_train, PredictTrain)

<span style="color: #dcaeea;">acc_test</span> = Accuracy(y_test, PredictTest)
<span style="color: #c678dd;">print</span>(acc_tr)
<span style="color: #c678dd;">print</span>(acc_test)
<span style="color: #c678dd;">print</span>(classification_report(y_test,PredictTest))





</pre>
</div>

<pre class="example" id="org4be33fa">
0.6859504132231405
0.6721311475409836
              precision    recall  f1-score   support

           0       0.68      0.48      0.57        27
           1       0.67      0.82      0.74        34

    accuracy                           0.67        61
   macro avg       0.68      0.65      0.65        61
weighted avg       0.67      0.67      0.66        61
</pre>




<div class="org-src-container">
<pre class="src src-python">
<span style="color: #dcaeea;">x</span> = raw_data[[<span style="color: #98be65;">'chol'</span>, <span style="color: #98be65;">'trestbps'</span>, <span style="color: #98be65;">'thalach'</span>]]
<span style="color: #dcaeea;">y</span> = raw_data.target
<span style="color: #dcaeea;">x_train</span>, <span style="color: #dcaeea;">x_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(x, y, test_size=<span style="color: #da8548; font-weight: bold;">0.2</span>, random_state=<span style="color: #da8548; font-weight: bold;">40</span>)
clf = GaussianNB()
clf.fit(x_train, y_train)
<span style="color: #c678dd;">print</span>(classification_report(y_test,clf.predict(x_test)))
</pre>
</div>

<pre class="example">
              precision    recall  f1-score   support

           0       0.67      0.62      0.64        26
           1       0.73      0.77      0.75        35

    accuracy                           0.70        61
   macro avg       0.70      0.69      0.70        61
weighted avg       0.70      0.70      0.70        61
</pre>
</div>
</div>
</div>



<div id="outline-container-org0c50c56" class="outline-2">
<h2 id="org0c50c56"><span class="section-number-2">3.</span> Questions</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-org42216a1" class="outline-3">
<h3 id="org42216a1"><span class="section-number-3">3.1.</span> Task 1</h3>
<div class="outline-text-3" id="text-3-1">
</div>
<div id="outline-container-orga26e4a4" class="outline-4">
<h4 id="orga26e4a4"><span class="section-number-4">3.1.1.</span> Q 5</h4>
<div class="outline-text-4" id="text-3-1-1">
<ul class="org-ul">
<li>A kernel is a function used in SVM for helping to solve problems. They provide shortcuts to avoid complex calculations. The amazing thing about kernel is that we can go to higher dimensions and perform smooth calculations with the help of it.</li>
<li>We can go up to an infinite number of dimensions using kernels.</li>
<li>Sometimes, we cannot have a hyperplane for certain problems. This problem arises when we go up to higher dimensions and try to form a hyperplane.</li>
<li>A kernel helps to form the hyperplane in the higher dimension without raising the complexity.</li>
<li>Overfitting happens when there are more feature sets than sample sets in the data. We can solve the problem by either increasing the data or by choosing the right kernel.</li>

<li>There are kernels like RBF that work well with smaller data as well. But, RBF is a universal kernel and using it on smaller datasets might increase the chances of overfitting.</li>

<li>Hence, the use of simpler kernels like linear and polynomial is encouraged.</li>
</ul>
</div>
</div>




<div id="outline-container-orgd6379b9" class="outline-4">
<h4 id="orgd6379b9"><span class="section-number-4">3.1.2.</span> Q 9 part B</h4>
<div class="outline-text-4" id="text-3-1-2">
<ul class="org-ul">
<li>One hot encoding is useful for data that has no relationship to each other. Machine learning algorithms treat the order of numbers as an attribute of significance. In other words, they will read a higher number as better or more important than a lower number.</li>

<li>While this is helpful for some ordinal situations, some input data does not have any ranking for category values, and this can lead to issues with predictions and poor performance. That’s when one hot encoding saves the day.</li>

<li>One hot encoding makes our training data more useful and expressive, and it can be rescaled easily. By using numeric values, we more easily determine a probability for our values. In particular, one hot encoding is used for our output values, since it provides more nuanced predictions than single labels.</li>
</ul>
</div>
</div>

<div id="outline-container-orged8eb15" class="outline-4">
<h4 id="orged8eb15"><span class="section-number-4">3.1.3.</span> Q 9 , part C</h4>
<div class="outline-text-4" id="text-3-1-3">
<ul class="org-ul">
<li>When our original continuous data do not follow the bell curve, we can transform this data to make it as “normal” as possible so that the statistical analysis results from this data become more valid. In other words, the transformation reduces or removes the skewness of our original data. The important caveat here is that the original data has to follow or approximately follow a log-normal distribution. Otherwise, the log transformation won’t work.</li>
</ul>
</div>
</div>





<div id="outline-container-org48d49cb" class="outline-4">
<h4 id="org48d49cb"><span class="section-number-4">3.1.4.</span> Q 11</h4>
<div class="outline-text-4" id="text-3-1-4">
<ul class="org-ul">
<li>The CART algorithm produces only binary Trees: non-leaf nodes always have two children (i.e., questions only have yes/no answers).</li>

<li>On the contrary, other Tree algorithms such as ID3 can produce Decision Trees with nodes having more than two children.</li>

<li>ID3 (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan. The algorithm creates a multiway tree, finding for each node (i.e. in a greedy manner) the categorical feature that will yield the largest information gain for categorical targets. Trees are grown to their maximum size and then a pruning step is usually applied to improve the ability of the tree to generalise to unseen data.</li>
</ul>


<ul class="org-ul">
<li>CART (Classification and Regression Trees) is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yields the largest information gain at each node.</li>
</ul>
</div>
</div>



<div id="outline-container-org173ca26" class="outline-4">
<h4 id="org173ca26"><span class="section-number-4">3.1.5.</span> Q 14</h4>
<div class="outline-text-4" id="text-3-1-5">
<ul class="org-ul">
<li>Pruning a decision tree helps to prevent overfitting the training data by reducing complexity so that our model generalizes well to unseen data. Pruning a decision tree means to remove a subtree that is redundant and not a useful split and replace it with a leaf node. Explainability — Pruned trees are shorter, simpler, and easier to explain.</li>
</ul>
</div>
</div>





<div id="outline-container-orge80284d" class="outline-4">
<h4 id="orge80284d"><span class="section-number-4">3.1.6.</span> Q 15</h4>
<div class="outline-text-4" id="text-3-1-6">
<ul class="org-ul">
<li>bootstrapping is a resampling technique that involves repeatedly drawing samples from our source data with replacement, often to estimate a population parameter. By “with replacement”, we mean that the same data point may be included in our resampled dataset multiple times.</li>
</ul>


<ul class="org-ul">
<li>Cross validation splits the available dataset to create multiple datasets, and Bootstrapping method uses the original dataset to create multiple datasets after resampling with replacement. Bootstrapping it is not as strong as Cross validation when it is used for model validation.</li>
</ul>
</div>
</div>

<div id="outline-container-org47fcaee" class="outline-4">
<h4 id="org47fcaee"><span class="section-number-4">3.1.7.</span> Q 16</h4>
<div class="outline-text-4" id="text-3-1-7">
<ul class="org-ul">
<li>2-fold cross-validation Split the data set into two halves train on one and test on the other. This gives you two results. This approach has been shown to work well if you do it five times. it&rsquo;s used when we do not have lot&rsquo;s of data for traning parameters .</li>
</ul>
</div>
</div>
</div>


<div id="outline-container-org31dab9b" class="outline-3">
<h3 id="org31dab9b"><span class="section-number-3">3.2.</span> Task 2</h3>
<div class="outline-text-3" id="text-3-2">
</div>
<div id="outline-container-orgd9a4797" class="outline-4">
<h4 id="orgd9a4797"><span class="section-number-4">3.2.1.</span> Q 1</h4>
<div class="outline-text-4" id="text-3-2-1">
<ul class="org-ul">
<li>Bayes&rsquo; Theorem states that the conditional probability of an event, based on the occurrence of another event, is equal to the likelihood of the second event given the first event multiplied by the probability of the first event.</li>

<li>Bayes Theorem provides a useful method for thinking about the relationship between a data set and a probability. In other words, the theorem says that the probability of a given hypothesis being true based on specific observed data can be stated as finding the probability of observing the data given the hypothesis multiplied by the probability of the hypothesis being true regardless of the data, divided by the probability of observing the data regardless of the hypothesis.</li>
</ul>


<ul class="org-ul">
<li>Naive Bayes classifier is a general term which refers to conditional independence of each of the features in the model.</li>

<li>Bernoulli Naive Bayes : It assumes that all our features are binary such that they take only two values. Means 0s can represent “word does not occur in the document” and 1s as &ldquo;word occurs in the document&rdquo; .</li>

<li>Multinomial Naive Bayes : Its is used when we have discrete data (e.g. movie ratings ranging 1 and 5 as each rating will have certain frequency to represent). In text learning we have the count of each word to predict the class or label.</li>

<li>Gaussian Naive Bayes : Because of the assumption of the normal distribution, Gaussian Naive Bayes is used in cases when all our features are continuous.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Mohammadb Ghorbani</p>
<p class="date">Created: 2022-05-19 Thu 20:36</p>
</div>
</body>
</html>
