{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project 2 - 400422108 - Sheedeh Sharif Bakhtiar.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project 2\n",
        "### Name: Sheedeh Sharif Bakhtiar\n",
        "### Student ID: 400422108\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **Dataset 1**\n",
        "\n",
        "### **Question 1**"
      ],
      "metadata": {
        "id": "2izrqu1wXme6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we import pandas and load our train dataframe:\n",
        "\n"
      ],
      "metadata": {
        "id": "Bk__gk4cWIEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"mobile_train.csv\")\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "78IS1rLuWPEA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "ece2fc96-7444-41db-cd5a-3848e0165495"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       battery_power       blue  clock_speed     dual_sim           fc  \\\n",
              "count    2000.000000  2000.0000  2000.000000  2000.000000  2000.000000   \n",
              "mean     1238.518500     0.4950     1.522250     0.509500     4.309500   \n",
              "std       439.418206     0.5001     0.816004     0.500035     4.341444   \n",
              "min       501.000000     0.0000     0.500000     0.000000     0.000000   \n",
              "25%       851.750000     0.0000     0.700000     0.000000     1.000000   \n",
              "50%      1226.000000     0.0000     1.500000     1.000000     3.000000   \n",
              "75%      1615.250000     1.0000     2.200000     1.000000     7.000000   \n",
              "max      1998.000000     1.0000     3.000000     1.000000    19.000000   \n",
              "\n",
              "            four_g   int_memory        m_dep    mobile_wt      n_cores  ...  \\\n",
              "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000  ...   \n",
              "mean      0.521500    32.046500     0.501750   140.249000     4.520500  ...   \n",
              "std       0.499662    18.145715     0.288416    35.399655     2.287837  ...   \n",
              "min       0.000000     2.000000     0.100000    80.000000     1.000000  ...   \n",
              "25%       0.000000    16.000000     0.200000   109.000000     3.000000  ...   \n",
              "50%       1.000000    32.000000     0.500000   141.000000     4.000000  ...   \n",
              "75%       1.000000    48.000000     0.800000   170.000000     7.000000  ...   \n",
              "max       1.000000    64.000000     1.000000   200.000000     8.000000  ...   \n",
              "\n",
              "         px_height     px_width          ram         sc_h         sc_w  \\\n",
              "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
              "mean    645.108000  1251.515500  2124.213000    12.306500     5.767000   \n",
              "std     443.780811   432.199447  1084.732044     4.213245     4.356398   \n",
              "min       0.000000   500.000000   256.000000     5.000000     0.000000   \n",
              "25%     282.750000   874.750000  1207.500000     9.000000     2.000000   \n",
              "50%     564.000000  1247.000000  2146.500000    12.000000     5.000000   \n",
              "75%     947.250000  1633.000000  3064.500000    16.000000     9.000000   \n",
              "max    1960.000000  1998.000000  3998.000000    19.000000    18.000000   \n",
              "\n",
              "         talk_time      three_g  touch_screen         wifi  price_range  \n",
              "count  2000.000000  2000.000000   2000.000000  2000.000000  2000.000000  \n",
              "mean     11.011000     0.761500      0.503000     0.507000     1.500000  \n",
              "std       5.463955     0.426273      0.500116     0.500076     1.118314  \n",
              "min       2.000000     0.000000      0.000000     0.000000     0.000000  \n",
              "25%       6.000000     1.000000      0.000000     0.000000     0.750000  \n",
              "50%      11.000000     1.000000      1.000000     1.000000     1.500000  \n",
              "75%      16.000000     1.000000      1.000000     1.000000     2.250000  \n",
              "max      20.000000     1.000000      1.000000     1.000000     3.000000  \n",
              "\n",
              "[8 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-489e99eb-a9d4-4919-83ae-7a91ad15bb8d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>battery_power</th>\n",
              "      <th>blue</th>\n",
              "      <th>clock_speed</th>\n",
              "      <th>dual_sim</th>\n",
              "      <th>fc</th>\n",
              "      <th>four_g</th>\n",
              "      <th>int_memory</th>\n",
              "      <th>m_dep</th>\n",
              "      <th>mobile_wt</th>\n",
              "      <th>n_cores</th>\n",
              "      <th>...</th>\n",
              "      <th>px_height</th>\n",
              "      <th>px_width</th>\n",
              "      <th>ram</th>\n",
              "      <th>sc_h</th>\n",
              "      <th>sc_w</th>\n",
              "      <th>talk_time</th>\n",
              "      <th>three_g</th>\n",
              "      <th>touch_screen</th>\n",
              "      <th>wifi</th>\n",
              "      <th>price_range</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.0000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1238.518500</td>\n",
              "      <td>0.4950</td>\n",
              "      <td>1.522250</td>\n",
              "      <td>0.509500</td>\n",
              "      <td>4.309500</td>\n",
              "      <td>0.521500</td>\n",
              "      <td>32.046500</td>\n",
              "      <td>0.501750</td>\n",
              "      <td>140.249000</td>\n",
              "      <td>4.520500</td>\n",
              "      <td>...</td>\n",
              "      <td>645.108000</td>\n",
              "      <td>1251.515500</td>\n",
              "      <td>2124.213000</td>\n",
              "      <td>12.306500</td>\n",
              "      <td>5.767000</td>\n",
              "      <td>11.011000</td>\n",
              "      <td>0.761500</td>\n",
              "      <td>0.503000</td>\n",
              "      <td>0.507000</td>\n",
              "      <td>1.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>439.418206</td>\n",
              "      <td>0.5001</td>\n",
              "      <td>0.816004</td>\n",
              "      <td>0.500035</td>\n",
              "      <td>4.341444</td>\n",
              "      <td>0.499662</td>\n",
              "      <td>18.145715</td>\n",
              "      <td>0.288416</td>\n",
              "      <td>35.399655</td>\n",
              "      <td>2.287837</td>\n",
              "      <td>...</td>\n",
              "      <td>443.780811</td>\n",
              "      <td>432.199447</td>\n",
              "      <td>1084.732044</td>\n",
              "      <td>4.213245</td>\n",
              "      <td>4.356398</td>\n",
              "      <td>5.463955</td>\n",
              "      <td>0.426273</td>\n",
              "      <td>0.500116</td>\n",
              "      <td>0.500076</td>\n",
              "      <td>1.118314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>501.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>256.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>851.750000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>109.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>282.750000</td>\n",
              "      <td>874.750000</td>\n",
              "      <td>1207.500000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1226.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>141.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>564.000000</td>\n",
              "      <td>1247.000000</td>\n",
              "      <td>2146.500000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1615.250000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>2.200000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>947.250000</td>\n",
              "      <td>1633.000000</td>\n",
              "      <td>3064.500000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1998.000000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>64.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1960.000000</td>\n",
              "      <td>1998.000000</td>\n",
              "      <td>3998.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-489e99eb-a9d4-4919-83ae-7a91ad15bb8d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-489e99eb-a9d4-4919-83ae-7a91ad15bb8d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-489e99eb-a9d4-4919-83ae-7a91ad15bb8d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we'll check for any null values in our dataframe:"
      ],
      "metadata": {
        "id": "gV0ZDumJYHaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUpy81OiYL0_",
        "outputId": "3f0ee05e-f2da-4083-9501-817515416589"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "battery_power    0\n",
              "blue             0\n",
              "clock_speed      0\n",
              "dual_sim         0\n",
              "fc               0\n",
              "four_g           0\n",
              "int_memory       0\n",
              "m_dep            0\n",
              "mobile_wt        0\n",
              "n_cores          0\n",
              "pc               0\n",
              "px_height        0\n",
              "px_width         0\n",
              "ram              0\n",
              "sc_h             0\n",
              "sc_w             0\n",
              "talk_time        0\n",
              "three_g          0\n",
              "touch_screen     0\n",
              "wifi             0\n",
              "price_range      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All seems to be well, so we don't need to handle null data. \n",
        "\n"
      ],
      "metadata": {
        "id": "Qx_2mIvbYUM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we're done with preprocessing, we'll split our dataframe into our features and our target value. Given that we're looking to predict the `price_range` value, all we need to do is to split our dataframe into two halves; one with all columns *but* `price_range`, and one dataframe *only* containing `price_range`.\n",
        "\n",
        "On top of that, to make things easier, as specified in the question, I'll be reducing the `price_range` number of classes down to 2: high and low. This will make things simpler down the line. If the `price_range` label is 0 or 1, it'll be set to 0 (low) and if the label is 2 or 3, it'll be set to 1 (high)\n",
        "\n",
        "I'll also be splitting the data into two halves: a train set and a test set. I'll set aside 20% of the data for testing purposes. I'll split the data using sklearn.\n",
        "\n"
      ],
      "metadata": {
        "id": "sfvpQO6lqks7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Reduce price_range down to two possible classes (rather than 4)\n",
        "df.loc[df['price_range'] == 1] = 0\n",
        "df.loc[df['price_range'] > 0] = 1\n",
        "\n",
        "# Split the data into feature/target\n",
        "X = df.drop(\"price_range\", axis=1)\n",
        "Y = df[\"price_range\"]\n",
        "\n",
        "# Split the now-scaled data into train/test sets\n",
        "train_x, test_x, train_y, test_y = train_test_split(\n",
        "    X, Y, test_size=0.2, random_state=0\n",
        ")\n"
      ],
      "metadata": {
        "id": "7quedvQAYtg4"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're looking to do forward selection to find the most effective combination of features with the highest ROC.\n",
        "\n",
        "In the book, **Introduction to Statistical Learning** by Gareth James, Forward (Stepwise) Selection is described as such:\n",
        "\n",
        "> Forward stepwise selection\n",
        "begins with a model containing no predictors, and then adds predictors\n",
        "to the model, one-at-a-time, until all of the predictors are in the model.\n",
        "In particular, at each step the variable that gives the greatest additional\n",
        "improvement to the fit is added to the model.\n",
        "\n",
        "Then, the best model out of all the models thus so far is chosen as our predictor model.\n",
        "\n",
        "I will be using vanilla logistic regression as our base model and I'll be using `Scikit-learn`'s implementation, documented here: [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
        "\n",
        "To do this, I will be making a list of column names, and at every step, I will add a single column to our model, until all columns have been added. \n",
        "\n",
        "For the null model, for the `X` value I will feed the model a single column the same shape as the `train_y` column consisting solely of zeros, taking care to set the `fit_intercept` parameter to `True`, effectively creating a null model that only fits an intercept regardless of any features.\n",
        "\n",
        "The strategy implemented in the following code is as such:\n",
        "\n",
        "0.   I'll first create a list of column names yet to be added to the model.\n",
        "1.   First we'll be testing the null model. For this, I'll make sure the `fit_intercept` parameter is set to `True` (despite `True` being the default value), and feed the classifier only zeros for the X parameter in `fit()`.\n",
        "2.  Then, I'll calculate the AUC using `scikit-learn`'s implementation as documented [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) in a tuple where the first element is the list of columns in our model, the second element corresponds to the AUC score and the third element is the model itself. This tuple will be added to a list, where the best model weights for every step is stored.\n",
        "3.  I'll then iterate over all columns that haven't been added to our model yet one at a time, always storing the best scoring model.\n",
        "4.  The best performing model from each step is added to the list, and step 3 is repeated until all features have a weight of 1.\n",
        "5.  The best performing model from the list is chosen.\n",
        "\n"
      ],
      "metadata": {
        "id": "rxacxdkmYvbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "# Get a list of columns\n",
        "columns_to_add = list(train_x.columns)\n",
        "\n",
        "# Results list here. Contains tuple of ([list of columns], AUC score)\n",
        "best_model_per_step = []\n",
        "\n",
        "# Train null model\n",
        "clf = LogisticRegression(fit_intercept=True)\n",
        "clf.fit(np.zeros(train_y.shape).reshape(-1, 1), train_y)  # Train null model\n",
        "null_predictions = clf.predict(np.zeros(test_y.shape).reshape(-1, 1))  # It doesn't quite matter what we pass for features in our null model, as it's only predicting based on the intercept that's independent of the features\n",
        "null_auc_score = roc_auc_score(test_y, null_predictions)\n",
        "best_model_per_step.append(([], null_auc_score, deepcopy(clf)))\n",
        "\n",
        "\n",
        "# Forward Stepwise Selection here.\n",
        "# The loop will keep going as long as there is one feature not included in the model\n",
        "added_columns = []\n",
        "while len(columns_to_add) > 0:\n",
        "  print(f\"Round: {len(added_columns)}/{len(list(train_x.columns))-1}\")\n",
        "  current_best_column = \"\"\n",
        "  current_best_auc = -1\n",
        "  current_best_model = -1\n",
        "\n",
        "  for column in columns_to_add:\n",
        "    # Choose proposed column\n",
        "    temp_columns = added_columns.copy()\n",
        "    temp_columns.append(column)\n",
        "\n",
        "    # Prepare data\n",
        "    columns_to_drop = columns_to_add.copy()\n",
        "    columns_to_drop.remove(column)\n",
        "\n",
        "    # Drop all columns except the ones we want to use in our \n",
        "    temp_train_x = train_x.drop(columns=columns_to_drop, axis=1)\n",
        "    temp_test_x = test_x.drop(columns=columns_to_drop, axis=1)\n",
        "\n",
        "    # Create and train proposed model\n",
        "    clf = LogisticRegression(fit_intercept=True)\n",
        "    clf.fit(temp_train_x, train_y)\n",
        "\n",
        "    # Evaluate current model for proposed added column\n",
        "    temp_pred_y = clf.predict(temp_test_x)\n",
        "    temp_auc = roc_auc_score(test_y, temp_pred_y, multi_class='ovr')\n",
        "\n",
        "    # Check and see if current proposed column is the best addition so far\n",
        "    if temp_auc > current_best_auc:\n",
        "      current_best_auc = temp_auc\n",
        "      current_best_column = column\n",
        "      current_best_model = deepcopy(clf)\n",
        "  \n",
        "  # Save column with the largest improvement among all other features\n",
        "  print(f\"Best column for this step: {current_best_column}\")\n",
        "  added_columns.append(current_best_column)\n",
        "  columns_to_add.remove(current_best_column)\n",
        "  best_model_per_step.append((added_columns.copy(), current_best_auc, deepcopy(current_best_model)))\n",
        "\n",
        "# Train final model with all features\n",
        "clf = LogisticRegression(fit_intercept=True)\n",
        "clf.fit(train_x, train_y)\n",
        "pred_y = clf.predict(test_x)\n",
        "full_auc = roc_auc_score(test_y, pred_y, multi_class='ovr')\n",
        "best_model_per_step.append((list(train_x.columns), full_auc, clf))\n",
        "\n",
        "best_model = max(best_model_per_step, key=lambda x:x[1])  # Pick the model with the highest AUC score\n",
        "print(f\"Best model columns: {best_model[0]} with AUC: {best_model[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxsRuMdOeSvQ",
        "outputId": "c8971156-3b05-422f-89fa-313b05782b33"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round: 0/19\n",
            "Best column for this step: m_dep\n",
            "Round: 1/19\n",
            "Best column for this step: battery_power\n",
            "Round: 2/19\n",
            "Best column for this step: blue\n",
            "Round: 3/19\n",
            "Best column for this step: clock_speed\n",
            "Round: 4/19\n",
            "Best column for this step: dual_sim\n",
            "Round: 5/19\n",
            "Best column for this step: fc\n",
            "Round: 6/19\n",
            "Best column for this step: four_g\n",
            "Round: 7/19\n",
            "Best column for this step: int_memory\n",
            "Round: 8/19\n",
            "Best column for this step: mobile_wt\n",
            "Round: 9/19\n",
            "Best column for this step: n_cores\n",
            "Round: 10/19\n",
            "Best column for this step: pc\n",
            "Round: 11/19\n",
            "Best column for this step: px_height\n",
            "Round: 12/19\n",
            "Best column for this step: px_width\n",
            "Round: 13/19\n",
            "Best column for this step: ram\n",
            "Round: 14/19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best column for this step: sc_h\n",
            "Round: 15/19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best column for this step: sc_w\n",
            "Round: 16/19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best column for this step: talk_time\n",
            "Round: 17/19\n",
            "Best column for this step: three_g\n",
            "Round: 18/19\n",
            "Best column for this step: touch_screen\n",
            "Round: 19/19\n",
            "Best column for this step: wifi\n",
            "Best model columns: ['m_dep', 'battery_power'] with AUC: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As seen above, the best performing model isn't the model with all features, but the model that only uses a subset of these features during training.\n",
        "\n",
        "\n",
        "\n",
        "### **Question 2**\n",
        "\n",
        "In this question, I'll be calculating the f1-score, recall and precision on our best performing model with features chosen using forward stepwise selection.\n",
        "\n",
        "Thankfully, `scikit-learn` has built-in methods that help with calculating and visualizing these metrics. Using `precision_recall_fscore_support` (as documented [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html)), I'll feed the best performing model's predictions and the true test data labels. In return, I'll get an array of length 4, where the first, second and third element correspond to precision, recall and f1-score respectively. (The other returned value is of no interest)"
      ],
      "metadata": {
        "id": "vaxLl6yXxZYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "best_clf = best_model[2]  # Get classifier model from tuple\n",
        "columns_chosen = best_model[0]\n",
        "\n",
        "# Change test data so that it fits the current classifier\n",
        "columns_to_drop = [col for col in list(test_x.columns) if col not in columns_chosen]\n",
        "fixed_test_x = test_x.drop(columns=columns_to_drop, axis=1)\n",
        "\n",
        "y_pred = best_clf.predict(fixed_test_x)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(test_y, y_pred, beta=1)\n",
        "\n",
        "print(f\"Precision:{precision}, recall:{recall}, f1_score:{f1_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeYrR_stzjuF",
        "outputId": "35480651-2836-4335-a8ac-70eaf8919051"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision:[1. 1.], recall:[1. 1.], f1_score:[1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 3**\n",
        "\n",
        "The number of columns needed in the best performing model is 2. Using `sklearn.decomposition.PCA` (as documented [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)), we can easily decompose our data."
      ],
      "metadata": {
        "id": "rFd_f0Wx3mIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=len(columns_chosen))\n",
        "pca.fit(X)\n",
        "print(pca.singular_values_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtEneAcY37yB",
        "outputId": "56d83ab6-ada6-494c-8226-6db6311a3982"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[36981.54912211 10277.32295916]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 4**\n",
        "\n",
        "For retraining the model on our new data, all we'll need to do is repeat what has been done in previous questions, just with new data."
      ],
      "metadata": {
        "id": "Gr8BVdVF4SN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_X = pca.transform(X)\n",
        "\n",
        "transformed_train_x, transformed_test_x, train_y, test_y = train_test_split(\n",
        "    transformed_X, Y, test_size=0.2, random_state=0\n",
        ")\n",
        "\n",
        "clf = LogisticRegression(fit_intercept=True)\n",
        "clf.fit(transformed_train_x, train_y)\n",
        "pred_y = clf.predict(transformed_test_x)\n",
        "\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(test_y, pred_y, beta=1)\n",
        "\n",
        "print(f\"Precision:{precision}, recall:{recall}, f1_score:{f1_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf3GDTK94hXP",
        "outputId": "f495498a-1477-4841-dbae-e6fe7af667ad"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision:[1.         0.69836066], recall:[0.50802139 1.        ], f1_score:[0.67375887 0.82239382]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 5: SVM Kernels**\n",
        "\n",
        "[As described here](https://www.geeksforgeeks.org/major-kernel-functions-in-support-vector-machine-svm/), simply put, kernel functions are methods used to take data as input and transform it into the required form of processing data.\n",
        "\n",
        "In **Introduction to Statistical Learning** by Gareth James, kernels in SVMs are described as enlarging the feature space in a certain way to accommodate a non-linear boundary between classes in our n-dimensional space that we wish to fit a hyperplane (Section 9.3.1 and 9.3.2).\n",
        "\n",
        "However, the \"trick\" is that kernels don't exactly enlargen our feature space how one might think; it allows us to operate in the *original* feature space without computing the coordinates of the data in a higher dimensional space. ([source](https://medium.com/@zxr.nju/what-is-the-kernel-trick-why-is-it-important-98a98db0961d))\n",
        "\n",
        "Different SVM algorithms use differing kinds of kernel functions. These functions are of different kinds—for instance, linear, nonlinear, polynomial, radial basis function (RBF), and sigmoid. \\ [source](https://dataaspirant.com/svm-kernels/#t-1608054630725)\n",
        "\n",
        "For instance, some other kernel functions and their use-cases are:\n",
        "\n",
        "\n",
        "*   **Linear**: They're arguably the simplest kernel function, and thus is the fastest kernel function. This function is preferable when speed is an important factor.\n",
        "> $k(x_i, x_j) = Σsum(x_i, x_j)$\n",
        "\n",
        "*   **Polynomial**: Popular in image processing. In the formula below, $d$ is the degree of the polynomial.\n",
        "> $k(x_i, x_j) = (x_i.x_j+1)^d$\n",
        "\n",
        "*   **Gaussian**: A general-purpose kernel, used when there is no prior knowledge about the data.\n",
        "> $k(x_{i},x_{j}) = exp(-\\gamma\\left\\|x_{i} - x_{j}\\right\\|^{2})$\n",
        "for $\\gamma>0$\n",
        "\n",
        "*   **Sigmoid**: Used in neural networks, it fits the data to a certain range and forms an 's' shape. This kernel function is similar to a two-layer perceptron model of the neural network, which works as an activation function for neurons.\n",
        "> $k(x, y) = tanh(\\alpha.x^{T}.y + c)$\n",
        "\n",
        "*   **Linear splines kernel in one-dimension**: It is useful when dealing with large sparse data vectors. It is often used in text categorization. The splines kernel also performs well in regression problems.\n",
        "> $k(x, y) = 1 + xy + xy.min(x, y) - \\frac{x+y}{2}min(x,y)^{2} + \\frac{1}{3}min(x,y)^{3}$\n",
        "\n",
        "*   **ANOVA/Radial basis function (RBF)**: The most preferred kind of kernel function, because it's localized and has a finite response along the complete x-axis. This function tends to perform well in multidimensional regression problems\n",
        "> $k(x, y) = \\Sigma_{k=1}^{n}exp(-σ(x^{k}-y^k)^2)^d$\n",
        "\n",
        "\n",
        "\n",
        "### **Question 6**\n",
        "\n",
        "Similar to previous questions, thanks to `scikit-learn`, the process of building and training an SVM model is incredibly simple. I'll be using [sklearn.svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) with the default parameters as they appear to suffice for this particular question."
      ],
      "metadata": {
        "id": "UNwVhJG-5P8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "rbf_clf = SVC()\n",
        "rbf_clf.fit(train_x, train_y)\n",
        "\n",
        "pred_y = rbf_clf.predict(test_x)\n",
        "acc = accuracy_score(test_y, pred_y)\n",
        "\n",
        "print(f\"SVM on mobile data with test accuracy: {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wVavnfm-jE3",
        "outputId": "c08f125c-6975-49e4-b89f-f7124f763dc4"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM on mobile data with test accuracy: 0.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 7**\n",
        "\n",
        "\n",
        "In `scikit-learn`, the default kernel in `sklearn.svm.SVC` is `'rbf'`. However, there are a number of kernels we can set for our classifier.\n",
        "\n",
        "The metric I'll be using to compare classifier with one another will be accuracy, as used in the previous question."
      ],
      "metadata": {
        "id": "c-emLl4_C4I5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "kernels = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
        "model_dict = {}\n",
        "\n",
        "for kr in kernels:\n",
        "  print(f\"Training SVM with kernel: {kr}\")\n",
        "  clf = SVC(kernel=kr)\n",
        "  clf.fit(train_x, train_y)\n",
        "  pred_y = clf.predict(test_x)\n",
        "  acc = accuracy_score(test_y, pred_y)\n",
        "  print(f\"Accuracy: {acc}\")\n",
        "  model_dict[kr] = [acc]\n",
        "\n",
        "# Create barplot\n",
        "kernel_df = pd.DataFrame.from_dict(model_dict)\n",
        "ax = sns.barplot(data=kernel_df)\n",
        "ax.set(xlabel=\"Kernel\", ylabel=\"Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "-IDJqtMrFjsi",
        "outputId": "b270f8a2-c311-4e5b-ea79-ae102b40442e"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SVM with kernel: linear\n",
            "Accuracy: 1.0\n",
            "Training SVM with kernel: poly\n",
            "Accuracy: 0.77\n",
            "Training SVM with kernel: rbf\n",
            "Accuracy: 0.77\n",
            "Training SVM with kernel: sigmoid\n",
            "Accuracy: 0.77\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 8: Soft Margins vs. Hard Margins**\n",
        "\n",
        "SVM tries to find the hyperplane with the largest margin with respect to the distances of the nearest sample of both classes.\n",
        "\n",
        "When the data is linearly separable, and we don’t want to have any misclassifications, we use SVM with a hard margin. However, when a linear boundary is not feasible, or we want to allow some misclassifications in the hope of achieving better generality, we can opt for a soft margin for our classifier.\n",
        "\n",
        "Sources:\n",
        "*   [baeldung.com](https://www.baeldung.com/cs/svm-hard-margin-vs-soft-margin)\n",
        "*   [section.io](https://www.section.io/engineering-education/using-a-hard-margin-vs-soft-margin-in-support-vector-machines/)\n",
        "*   [Sharif University](http://ce.sharif.edu/courses/91-92/2/ce725-2/resources/root/Lectures/Support%20Vector%20Machine%20(SVM).pdf)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **Question 9**\n",
        "\n",
        "Same as the previous questions, we'll be using `scikit-learn` to solve the problems outlined in this question. \n",
        "\n",
        "#### *A) Binning on `battery_power` with at least 3 bins*"
      ],
      "metadata": {
        "id": "T35BIXMuKJ5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "N_BINS = 3\n",
        "\n",
        "X_bin = X.copy()\n",
        "binner = KBinsDiscretizer(n_bins=N_BINS, encode='ordinal', strategy='kmeans') # With this strategy, bins might not be the same width\n",
        "binner.fit(X_bin['battery_power'].values.reshape(-1, 1))\n",
        "X_bin['battery_power'] = binner.transform(X_bin['battery_power'].values.reshape(-1, 1))\n",
        "X_bin['battery_power'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGV3fsGwOQ3O",
        "outputId": "357eb382-7859-4a3a-e24e-674b27da11d2"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    2000.000000\n",
              "mean        0.359000\n",
              "std         0.669584\n",
              "min         0.000000\n",
              "25%         0.000000\n",
              "50%         0.000000\n",
              "75%         0.250000\n",
              "max         2.000000\n",
              "Name: battery_power, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *B) One-hot encoding on categorical features*"
      ],
      "metadata": {
        "id": "wx8u2CuRRNDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Features to transform\n",
        "categorical_features = ['blue', 'dual_sim', 'four_g', 'three_g', 'touch_screen', 'wifi']\n",
        "\n",
        "X_one_hot = X.copy()\n",
        "X_temp = X.copy()\n",
        "\n",
        "# Create temp DataFrame where we'll tranform all categorical features at once.\n",
        "columns_to_drop = [col for col in list(X.columns) if col not in categorical_features]\n",
        "X_temp = X_temp.drop(columns=columns_to_drop, axis=1)\n",
        "\n",
        "\n",
        "enc = OneHotEncoder().fit(X_temp)\n",
        "new_X_temp = enc.transform(X_temp)\n",
        "\n",
        "\n",
        "# Add encoded features\n",
        "for idx, feature in enumerate(categorical_features):\n",
        "  for i in range(2):\n",
        "    new_col = new_X_temp[:,idx+i].toarray()\n",
        "    X_one_hot[f\"{feature}-{i}\"] = new_col\n",
        "\n",
        "# Drop original categorical columns\n",
        "X_one_hot.drop(columns=categorical_features, axis=1)\n",
        "\n",
        "X_one_hot.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "Ji-_mn6xRadX",
        "outputId": "cb9356fb-4014-4a77-84ed-69db5bc36959"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  m_dep  \\\n",
              "0              0     0          0.0         0   0       0           0    0.0   \n",
              "1              1     1          1.0         1   1       1           1    1.0   \n",
              "2              1     1          1.0         1   1       1           1    1.0   \n",
              "3              1     1          1.0         1   1       1           1    1.0   \n",
              "4              0     0          0.0         0   0       0           0    0.0   \n",
              "\n",
              "   mobile_wt  n_cores  ...  dual_sim-0  dual_sim-1  four_g-0  four_g-1  \\\n",
              "0          0        0  ...         0.0         1.0       1.0       0.0   \n",
              "1          1        1  ...         1.0         0.0       0.0       1.0   \n",
              "2          1        1  ...         1.0         0.0       0.0       1.0   \n",
              "3          1        1  ...         1.0         0.0       0.0       1.0   \n",
              "4          0        0  ...         0.0         1.0       1.0       0.0   \n",
              "\n",
              "   three_g-0  three_g-1  touch_screen-0  touch_screen-1  wifi-0  wifi-1  \n",
              "0        0.0        1.0             1.0             0.0     0.0     1.0  \n",
              "1        1.0        0.0             0.0             1.0     1.0     0.0  \n",
              "2        1.0        0.0             0.0             1.0     1.0     0.0  \n",
              "3        1.0        0.0             0.0             1.0     1.0     0.0  \n",
              "4        0.0        1.0             1.0             0.0     0.0     1.0  \n",
              "\n",
              "[5 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-316a30da-b3f0-4ab9-b082-b1c74c29ed98\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>battery_power</th>\n",
              "      <th>blue</th>\n",
              "      <th>clock_speed</th>\n",
              "      <th>dual_sim</th>\n",
              "      <th>fc</th>\n",
              "      <th>four_g</th>\n",
              "      <th>int_memory</th>\n",
              "      <th>m_dep</th>\n",
              "      <th>mobile_wt</th>\n",
              "      <th>n_cores</th>\n",
              "      <th>...</th>\n",
              "      <th>dual_sim-0</th>\n",
              "      <th>dual_sim-1</th>\n",
              "      <th>four_g-0</th>\n",
              "      <th>four_g-1</th>\n",
              "      <th>three_g-0</th>\n",
              "      <th>three_g-1</th>\n",
              "      <th>touch_screen-0</th>\n",
              "      <th>touch_screen-1</th>\n",
              "      <th>wifi-0</th>\n",
              "      <th>wifi-1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 32 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-316a30da-b3f0-4ab9-b082-b1c74c29ed98')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-316a30da-b3f0-4ab9-b082-b1c74c29ed98 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-316a30da-b3f0-4ab9-b082-b1c74c29ed98');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *C) Log transformation*\n",
        "\n",
        "Log transformation is a data transformation method in which it replaces each variable x with a log(x). The choice of the logarithm base is usually left up to the analyst and it would depend on the purposes of statistical modeling. ([source](https://medium.com/@kyawsawhtoon/log-transformation-purpose-and-interpretation-9444b4b049c9))\n",
        "\n",
        "Log transformations should be used when our data is skewed and cannot be fixed by filtering outliers, in an attempt to normalize our data and have it fit the bell curve. \n",
        "\n",
        "In general, you take logs of a variable when the relationships to that variable are multiplicative rather than additive to remove said skewedness ([source](https://stats.stackexchange.com/questions/82641/how-do-i-know-when-i-should-use-a-log-transformation-on-a-variable-by-multiple-r)), so just by checking which quantitive values are multiplicative should be enough for us to know which values we should log-transform.\n",
        "\n",
        "For this dataset, we'll use `scipy`'s `skewtest` method (documented [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.skewtest.html#scipy.stats.skewtest)) which returns a p-value and z-score relative to a nominal distribution and set a certain threshold that if the calculated p-value is above a certain threshold, we'll perform a log transform."
      ],
      "metadata": {
        "id": "_VJhHAfgWkr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import skewtest\n",
        "\n",
        "P_VALUE_THRES = 0.001\n",
        "\n",
        "non_categorical_X = X.copy()\n",
        "non_categorical_X = non_categorical_X.drop(columns=categorical_features, axis=1)\n",
        "\n",
        "for col in list(non_categorical_X.columns):\n",
        "  current_col = non_categorical_X[col]\n",
        "  skew_res = skewtest(current_col)\n",
        "  print(f\"For column {col}: {skew_res}\")\n",
        "  if skew_res.pvalue > P_VALUE_THRES:\n",
        "    print(\"Data is skewed!\")\n",
        "    non_categorical_X.loc[col] = non_categorical_X[col].apply(lambda x: log(x)) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8_OxFZXZjix",
        "outputId": "d14a1a76-0509-47a5-9aee-20785477e551"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For column battery_power: SkewtestResult(statistic=22.01490511597988, pvalue=2.0730427451762666e-107)\n",
            "For column clock_speed: SkewtestResult(statistic=12.937742340445343, pvalue=2.7561021191648625e-38)\n",
            "For column fc: SkewtestResult(statistic=31.365405982789795, pvalue=5.998581519982016e-216)\n",
            "For column int_memory: SkewtestResult(statistic=24.814140204086456, pvalue=6.3089741243293125e-136)\n",
            "For column m_dep: SkewtestResult(statistic=-8.491491454687079, pvalue=2.040019311227217e-17)\n",
            "For column mobile_wt: SkewtestResult(statistic=19.833983170752532, pvalue=1.5154988667224644e-87)\n",
            "For column n_cores: SkewtestResult(statistic=23.098488672479508, pvalue=4.794777877040834e-118)\n",
            "For column pc: SkewtestResult(statistic=25.144792613418527, pvalue=1.6112536569945215e-139)\n",
            "For column px_height: SkewtestResult(statistic=27.53119326805456, pvalue=7.433398358101139e-167)\n",
            "For column px_width: SkewtestResult(statistic=21.70628795039023, pvalue=1.7893935750941816e-104)\n",
            "For column ram: SkewtestResult(statistic=23.97953965766978, pvalue=4.546791548704267e-127)\n",
            "For column sc_h: SkewtestResult(statistic=21.2409415200846, pvalue=3.997962358142863e-100)\n",
            "For column sc_w: SkewtestResult(statistic=27.71229526928632, pvalue=4.964242460664233e-169)\n",
            "For column talk_time: SkewtestResult(statistic=24.06294952583955, pvalue=6.110066579571281e-128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thankfully, all of our data has a nominal distribution with incredibly small p-values, so no log-transformation is needed.\n",
        "\n",
        "#### *D) Adding features*\n",
        "\n",
        "To add a new feature `sc_area`, all we need to do is to multiply each element of `sc_h` by `sc_w` and add it as a column to our dataframe, which is incredibly simple to do."
      ],
      "metadata": {
        "id": "KgYR-zLnjnJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc_area_col = X['sc_h'] * X['sc_w']\n",
        "\n",
        "with_area_X = X.copy()\n",
        "with_area_X['sc_area'] = sc_area_col\n",
        "\n",
        "with_area_X.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "IRsyYACmkKs7",
        "outputId": "d97a7acd-69c4-4f82-962a-9cfd2f2f5115"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       battery_power         blue  clock_speed     dual_sim           fc  \\\n",
              "count    2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
              "mean      279.725500     0.621500     0.887550     0.625000     1.521000   \n",
              "std       525.228184     0.485134     0.702208     0.484244     2.593631   \n",
              "min         0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%         0.750000     0.000000     0.375000     0.000000     0.000000   \n",
              "50%         1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "75%       126.500000     1.000000     1.000000     1.000000     1.000000   \n",
              "max      1994.000000     1.000000     3.000000     1.000000    19.000000   \n",
              "\n",
              "            four_g   int_memory        m_dep    mobile_wt      n_cores  ...  \\\n",
              "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000  ...   \n",
              "mean      0.629500     8.293500     0.622550    35.638000     1.650000  ...   \n",
              "std       0.483059    16.017136     0.439685    63.256096     2.086547  ...   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
              "25%       0.000000     0.750000     0.075000     0.750000     0.750000  ...   \n",
              "50%       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
              "75%       1.000000     1.250000     1.000000    20.750000     1.000000  ...   \n",
              "max       1.000000    64.000000     1.000000   200.000000     8.000000  ...   \n",
              "\n",
              "         px_height     px_width          ram         sc_h         sc_w  \\\n",
              "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
              "mean    134.602000   288.067500   196.828500     3.581000     1.920500   \n",
              "std     297.557428   538.980952   385.155563     5.484294     3.038881   \n",
              "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
              "25%       0.750000     0.750000     0.750000     0.750000     0.000000   \n",
              "50%       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
              "75%       1.000000   125.750000    64.750000     2.000000     1.000000   \n",
              "max    1878.000000  1989.000000  1974.000000    19.000000    18.000000   \n",
              "\n",
              "         talk_time      three_g  touch_screen         wifi      sc_area  \n",
              "count  2000.000000  2000.000000   2000.000000  2000.000000  2000.000000  \n",
              "mean      3.153000     0.686500      0.631000     0.624000    20.138000  \n",
              "std       5.098293     0.464032      0.482655     0.484501    50.004191  \n",
              "min       0.000000     0.000000      0.000000     0.000000     0.000000  \n",
              "25%       0.750000     0.000000      0.000000     0.000000     0.000000  \n",
              "50%       1.000000     1.000000      1.000000     1.000000     1.000000  \n",
              "75%       1.250000     1.000000      1.000000     1.000000     1.000000  \n",
              "max      20.000000     1.000000      1.000000     1.000000   342.000000  \n",
              "\n",
              "[8 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b93c8436-072c-4612-a953-145b0f852e74\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>battery_power</th>\n",
              "      <th>blue</th>\n",
              "      <th>clock_speed</th>\n",
              "      <th>dual_sim</th>\n",
              "      <th>fc</th>\n",
              "      <th>four_g</th>\n",
              "      <th>int_memory</th>\n",
              "      <th>m_dep</th>\n",
              "      <th>mobile_wt</th>\n",
              "      <th>n_cores</th>\n",
              "      <th>...</th>\n",
              "      <th>px_height</th>\n",
              "      <th>px_width</th>\n",
              "      <th>ram</th>\n",
              "      <th>sc_h</th>\n",
              "      <th>sc_w</th>\n",
              "      <th>talk_time</th>\n",
              "      <th>three_g</th>\n",
              "      <th>touch_screen</th>\n",
              "      <th>wifi</th>\n",
              "      <th>sc_area</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>2000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>279.725500</td>\n",
              "      <td>0.621500</td>\n",
              "      <td>0.887550</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>1.521000</td>\n",
              "      <td>0.629500</td>\n",
              "      <td>8.293500</td>\n",
              "      <td>0.622550</td>\n",
              "      <td>35.638000</td>\n",
              "      <td>1.650000</td>\n",
              "      <td>...</td>\n",
              "      <td>134.602000</td>\n",
              "      <td>288.067500</td>\n",
              "      <td>196.828500</td>\n",
              "      <td>3.581000</td>\n",
              "      <td>1.920500</td>\n",
              "      <td>3.153000</td>\n",
              "      <td>0.686500</td>\n",
              "      <td>0.631000</td>\n",
              "      <td>0.624000</td>\n",
              "      <td>20.138000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>525.228184</td>\n",
              "      <td>0.485134</td>\n",
              "      <td>0.702208</td>\n",
              "      <td>0.484244</td>\n",
              "      <td>2.593631</td>\n",
              "      <td>0.483059</td>\n",
              "      <td>16.017136</td>\n",
              "      <td>0.439685</td>\n",
              "      <td>63.256096</td>\n",
              "      <td>2.086547</td>\n",
              "      <td>...</td>\n",
              "      <td>297.557428</td>\n",
              "      <td>538.980952</td>\n",
              "      <td>385.155563</td>\n",
              "      <td>5.484294</td>\n",
              "      <td>3.038881</td>\n",
              "      <td>5.098293</td>\n",
              "      <td>0.464032</td>\n",
              "      <td>0.482655</td>\n",
              "      <td>0.484501</td>\n",
              "      <td>50.004191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>126.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>20.750000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>125.750000</td>\n",
              "      <td>64.750000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1994.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>64.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1878.000000</td>\n",
              "      <td>1989.000000</td>\n",
              "      <td>1974.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>342.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b93c8436-072c-4612-a953-145b0f852e74')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b93c8436-072c-4612-a953-145b0f852e74 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b93c8436-072c-4612-a953-145b0f852e74');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 10** \n",
        "\n",
        "\n",
        "First, we'll fit an SVM onto each of our generated dataframes from the previous question:"
      ],
      "metadata": {
        "id": "MJaHG_fLkm2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rbf_clf = SVC()\n",
        "results = {}\n",
        "\n",
        "\n",
        "# A) Binning\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(\n",
        "    X_bin, Y, test_size=0.2, random_state=0\n",
        ")\n",
        "rbf_clf.fit(train_x, train_y)\n",
        "pred_y_bin = rbf_clf.predict(test_x)\n",
        "acc_bin = accuracy_score(test_y, pred_y_bin)\n",
        "results['bin'] = [acc_bin]\n",
        "print(f\"Bin accuracy: {acc_bin}\")\n",
        "\n",
        "# B) One-hot encoding\n",
        "train_x, test_x, train_y, test_y = train_test_split(\n",
        "    X_one_hot, Y, test_size=0.2, random_state=0\n",
        ")\n",
        "rbf_clf.fit(train_x, train_y)\n",
        "pred_y_oh = rbf_clf.predict(test_x)\n",
        "acc_oh = accuracy_score(test_y, pred_y_oh)\n",
        "results['one_hot'] = [acc_oh]\n",
        "print(f\"One hot accuracy: {acc_oh}\")\n",
        "\n",
        "\n",
        "\n",
        "# C) Log-transformation\n",
        "#### No changes were done\n",
        "\n",
        "# D) Feature adding\n",
        "train_x, test_x, train_y, test_y = train_test_split(\n",
        "    with_area_X, Y, test_size=0.2, random_state=0\n",
        ")\n",
        "rbf_clf.fit(train_x, train_y)\n",
        "pred_y_area = rbf_clf.predict(test_x)\n",
        "acc_area = accuracy_score(test_y, pred_y_area)\n",
        "results['area'] = [acc_area]\n",
        "print(f\"SC_area accuracy: {acc_area}\")\n",
        "\n",
        "\n",
        "# Create barplot\n",
        "res_df = pd.DataFrame.from_dict(results)\n",
        "ax = sns.barplot(data=res_df)\n",
        "ax.set(xlabel=\"Feature Engineering Type\", ylabel=\"Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "WSla01h2kuf8",
        "outputId": "24fb3350-7c28-4632-9732-2b6f441b854c"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bin accuracy: 0.77\n",
            "One hot accuracy: 0.77\n",
            "SC_area accuracy: 0.77\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we'll combine all the altered dataframes to create a dataframe where the features have been engineered as per the previous question and fit a SVM onto it:"
      ],
      "metadata": {
        "id": "JtjuJZ5Jku61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alt_X = X.copy()\n",
        "\n",
        "# First, removing categorical columns and adding the encoded columns\n",
        "# Columns not in X_one_hot but in alt_X are the original, non-encoded columns and should be removed,\n",
        "# and columns in X_one_hot but not in alt_X are the encoded columns and should be added\n",
        "org_columns = list(alt_X.columns)\n",
        "new_columns = list(X_one_hot.columns)\n",
        "\n",
        "columns_to_drop = [col for col in org_columns if col not in new_columns]\n",
        "alt_X = alt_X.drop(columns=columns_to_drop, axis=1)\n",
        "\n",
        "columns_to_add = [col for col in new_columns if col not in org_columns]\n",
        "for col in columns_to_add:\n",
        "  alt_X[col] = X_one_hot[col]\n",
        "\n",
        "# Then, adding the binned data\n",
        "alt_X['battery_power'] = X_bin['battery_power']\n",
        "\n",
        "# Lastly, adding the screen area column\n",
        "alt_X['sc_area'] = with_area_X['sc_area']\n",
        "\n",
        "# Train/Test split the data and finally, train our SVM\n",
        "clf = SVC()\n",
        "train_x, test_x, train_y, test_y = train_test_split(\n",
        "    alt_X, Y, test_size=0.2, random_state=0\n",
        ")\n",
        "clf.fit(train_x, train_y)\n",
        "pred_y = clf.predict(test_x)\n",
        "final_acc = accuracy_score(test_y, pred_y)\n",
        "\n",
        "print(f\"SVM accuracy with all engineered features: {final_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ffbKpi3k9Z_",
        "outputId": "2f8b2a7e-b21a-4e6a-f660-42e3250ed432"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM accuracy with all engineered features: 0.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 11: Decision Tree Algorthms** \n",
        "\n",
        "There are a few different algorithms for deriving decision trees. Here are some of them:\n",
        "\n",
        "1. **ID3 (Iterative Dichotomiser)**: This algorithm creats a multiway tree. It uses Information Gain as attribute selection measure and has a greedy approach. After trees have been grown to their maximum size, a pruning step is usually applied to improve the ability of the tree to generalise to unseen data.\n",
        "\n",
        "2. **C4.5**:  The successor to ID3, this algorithm removes the restriction that features must be categorical by dynamically defining a discrete attribute based on numerical variables that partitions the continuous attribute value into a discrete set of intervals. This algorithm converts trained trees into sets of if-then rules. Pruning is done by removing a rule’s precondition if the accuracy of the rule improves without it. Uses Gain Ratio as attribute selection measure.\n",
        "\n",
        "3. **CART (Classification and Regression Trees)**: Very similar to C4.5, this is a greedy algorithm that searches for an optimum split at the top level, and then repeats this process for each of the subsequent levels. Moreover, it does verify whether the split will lead to the lowest impurity or not as well as the solution provided by the greedy algorithm is not guaranteed to be optimal, it often produces a solution that’s reasonably good since finding the optimal Tree is an NP-Complete problem that requires exponential time complexity. This algorithm uses the Gini Index as attribute selection measure.\n",
        "\n",
        "\n",
        "Sources:\n",
        "*   [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/05/25-questions-to-test-your-skills-on-decision-trees/)\n",
        "*   [medium](https://medium.datadriveninvestor.com/tree-algorithms-id3-c4-5-c5-0-and-cart-413387342164)\n",
        "\n",
        "\n",
        "### **Question 12** \n",
        "\n",
        "Similar to previous questions, I'll be using `sklearn` to build a decision tree. Specifically, I'll be using the `DecisionTreeClassifer` as documented [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)."
      ],
      "metadata": {
        "id": "-6ZU8vGbzy73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf = clf.fit(train_x, train_y)\n",
        "\n",
        "pred_y = clf.predict(test_x)\n",
        "tree_acc = accuracy_score(test_y, pred_y)\n",
        "\n",
        "print(f\"Decision tree accuracy: {tree_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9YvWdKU1x04",
        "outputId": "1310e561-80f2-4550-9f89-f27f3824901a"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision tree accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 13** \n",
        "\n",
        "There are several parameters we can evaluate and compare when it comes to decision trees. The first of which would be the splitting criterion.\n",
        "\n",
        "#### *Splitting Criterion*\n",
        "`scikit-learn`'s `DecisionTreeClassifier` offers three different types of criterion for measuring the quality of a split. They are gini, entropy, and log loss, with gini being the default value.\n",
        "\n",
        "**Gini impurity** ranges values from 0 to 0.5 (with 0 being the best and 0.5 being the \"worst\"), and the formula is as shown below:\n",
        "\n",
        "$Gini(t) = 1 - \\Sigma_{i=1}^jP(i|t)^2$\n",
        "\n",
        "Where $j$ represents the number of classes in the label and $P$ represents the ratio of class at the $i$th node. A gini index of 0 suggests that a node is a leaf and it is the purest node.\n",
        "\n",
        "The **entropy** of any random variable or random process is the average level of uncertainty involved in the possible outcome of the variable or process. Mathematically, the formula for entropy is shown below:\n",
        "\n",
        "$Entropy(X) = -\\Sigma_{i=1}^nlogP(x_i)$\n",
        "\n",
        "Where $X$ is the random variable or process, $X_i$ is the possible outcome or class, and $P(X_i)$ is the probability for that specific class.\n",
        "\n",
        "**Log-loss** on the other hand, is indicative of how close the prediction probability is to the corresponding actual/true value (0 or 1 in case of binary classification). The more the predicted probability diverges from the actual value, the higher is the log-loss value. Log-loss is calculated using the formula below:\n",
        "\n",
        "$LogLoss(X) = -\\left [ y_ilnp_i + (1-y_i)ln(1-p_i)\\right ]$\n",
        "\n",
        "Where $i$ is the given observation/record, $y$ is the actual/true value, $p$ is the prediction probability, and $ln$ refers to the natural logarithm of a number.\n",
        "\n",
        "The performance of these criteria is tested and graphed below:\n"
      ],
      "metadata": {
        "id": "JQt1h5uw2S7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_res = {}\n",
        "criterion = [\"gini\", \"entropy\"]\n",
        "\n",
        "for criteria in criterion:\n",
        "  clf = DecisionTreeClassifier(criterion=criteria)\n",
        "  clf = clf.fit(train_x, train_y)\n",
        "\n",
        "  pred_y = clf.predict(test_x)\n",
        "  tree_acc = accuracy_score(test_y, pred_y)\n",
        "  criterion_res[criteria] = [tree_acc]\n",
        "\n",
        "\n",
        "# Create barplot\n",
        "criteria_df = pd.DataFrame.from_dict(criterion_res)\n",
        "ax = sns.barplot(data=criteria_df)\n",
        "ax.set(xlabel=\"Criterion\", ylabel=\"Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "y7GFhX0P401L",
        "outputId": "7a27f476-fea0-46f0-cc75-5b401af0f028"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### *Max Depth*\n",
        "\n",
        "As per `scikit-learn`'s documentation, `DecisionTreeClassifier` has a `max_depth` parameter that specifies the maximum depth of the tree. If `None`, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
        "\n",
        "We'll test our model's accuracy for max depths of 1, 5, 10, 20, 50 and `None` (infinity)."
      ],
      "metadata": {
        "id": "HJO_cV-c40Wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_depths = [1, 5, 10, 20, 50, None]\n",
        "depth_res = {}\n",
        "\n",
        "for depth in sample_depths:\n",
        "  clf = DecisionTreeClassifier(max_depth=depth)\n",
        "  clf = clf.fit(train_x, train_y)\n",
        "\n",
        "  pred_y = clf.predict(test_x)\n",
        "  depth_acc = accuracy_score(test_y, pred_y)\n",
        "  depth_res[str(depth)] = [depth_acc]\n",
        "\n",
        "\n",
        "# Create barplot\n",
        "depth_df = pd.DataFrame.from_dict(depth_res)\n",
        "ax = sns.barplot(data=depth_df)\n",
        "ax.set(xlabel=\"Depth\", ylabel=\"Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Elzwa6Jx42PK",
        "outputId": "3a557fae-10d9-46fa-ae19-2f56c8abdad6"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### *Minimum Samples to Split*\n",
        "\n",
        "As per `scikit-learn`'s documentation, the `min_samples_split` paramter refers to the minimum number of samples required to split an internal node. It's default value is 2. I'll be testing for paramter values 2, 3, 5 and 10.\n"
      ],
      "metadata": {
        "id": "Fn9fEdB-42qD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_split = [2, 3, 5, 10]\n",
        "split_res = {}\n",
        "\n",
        "for split in sample_split:\n",
        "  clf = DecisionTreeClassifier(min_samples_split=split)\n",
        "  clf = clf.fit(train_x, train_y)\n",
        "\n",
        "  pred_y = clf.predict(test_x)\n",
        "  split_acc = accuracy_score(test_y, pred_y)\n",
        "  split_res[str(split)] = [split_acc]\n",
        "\n",
        "\n",
        "# Create barplot\n",
        "split_df = pd.DataFrame.from_dict(split_res)\n",
        "ax = sns.barplot(data=split_df)\n",
        "ax.set(xlabel=\"Min Split\", ylabel=\"Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "S5jOjcSK79at",
        "outputId": "6456e0c9-ada2-4036-fb6b-50cb4ea060e4"
      },
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 14: Pruning and what is it used for** \n",
        "\n",
        "One of the questions that arises in a decision tree algorithm is the optimal size of the final tree. A tree that is too large risks overfitting the training data and poorly generalizing to new samples. A small tree might not capture important structural information about the sample space. However, it is hard to tell when a tree algorithm should stop because it is impossible to tell if the addition of a single extra node will dramatically decrease error. This problem is known as the horizon effect. A common strategy is to grow the tree until each node contains a small number of instances then use pruning to remove nodes that do not provide additional information\n",
        "\n",
        "Pruning a decision tree helps to prevent overfitting the training data so that our model generalizes well to unseen data. Pruning a decision tree means to remove a subtree that is redundant and not a useful split and replace it with a leaf node. Decision tree pruning can be divided into two types: pre-pruning and post-pruning.\n",
        "\n",
        "Pre-prunning is when the subtree construction is halted at a particular node after some kind of evaluation, such as Gini Impurity or Information gain. Post-pruning is pruning after the tree is built, working from the bottom-up and based on various measures such as Gini Impurity or Information Gain, deciding whether or not a certain node should be replaced with a leaf node.\n",
        "\n",
        "Pruning and early stopping help quite a bit with the trade-offs involved when using algorithms such as CART.\n",
        "\n",
        "Sources:\n",
        "*   [educative.io](https://www.educative.io/edpresso/what-is-decision-tree-pruning-and-how-is-it-done)\n",
        "*   [wikipedia](https://en.wikipedia.org/wiki/Decision_tree_pruning)\n",
        "*   [displayr.com](https://www.displayr.com/machine-learning-pruning-decision-trees/)\n",
        "\n",
        "\n",
        "### **Question 15: Bootstrapping and it's difference with cross validation** \n",
        "In statistics, Bootstrap Sampling is a method that involves drawing of sample data repeatedly with replacement from a data source to estimate a population parameter. By “with replacement”, we mean that the same data point may be included in our resampled dataset multiple times.\n",
        "\n",
        "In general, the steps involved in bootstrapping are as such:\n",
        "\n",
        "1.  Choose a number of bootstrap samples to perform\n",
        "2.  Choose a sample size\n",
        "3.  For each bootstrap sample \n",
        "  1.  Draw a sample with replacement with the chosen size\n",
        "  2.  Fit a model on the data sample\n",
        "  3.  Estimate the skill of the model on the out-of-bag sample.\n",
        "4.  Calculate the mean of the sample of model skill estimates.\n",
        "\n",
        "Bootstrapping allows us to generate a distribution of estimates, rather than a single point estimate and it allows us to estimate uncertainty, allowing computation of confidence intervals.\n",
        "\n",
        "However, it is commonly confused with cross-validation. While cross-validation is also a resampling method, there are a couple key differences:\n",
        "*  Bootstrap resamples with replacement. Due to the drawing with replacement, a bootstrapped data set may contain multiple instances of the same original cases, and may completely omit other original cases. However, cross-validation resamples without replacement and produces new datasets smaller than the original.\n",
        "*  As the name cross validation suggests, its primary purpose is measuring (generalization) performance of a model. On contrast, bootstrapping is primarily used to establish empirical distribution functions for a widespread range of statistics (widespread as in ranging from, say, the variation of the mean to the variation of models in bagged ensemble models).\n",
        "\n",
        "Sources:\n",
        "*   [analyticsvidhya.com](https://www.analyticsvidhya.com/blog/2020/02/what-is-bootstrap-sampling-in-statistics-and-machine-learning/)\n",
        "*   [carpentries-incubator on GitHub](https://carpentries-incubator.github.io/machine-learning-novice-python/08-bootstrapping/index.html)\n",
        "*   [machinelearningmastery.com](https://machinelearningmastery.com/a-gentle-introduction-to-the-bootstrap-method/)\n",
        "*  [stackexchange](https://datascience.stackexchange.com/questions/32264/what-is-the-difference-between-bootstrapping-and-cross-validation)\n",
        "\n",
        "### **Question 16: 5x2 cross validation and it's uses** \n",
        "\n",
        "Nested cross-validation (CV) is often used to train a model in which hyperparameters also need to be optimized. Nested CV estimates the generalization error of the underlying model and its (hyper)parameter search.\n",
        "\n",
        "In simpler terms, it's validation for our validation set, or rather, inner-validation.\n",
        "\n",
        "This procedure can be used both when optimizing the hyperparameters of a model on a dataset, and when comparing and selecting a model for the dataset. When the same cross-validation procedure and dataset are used to both tune and select a model, it is likely to lead to an optimistically biased evaluation of the model performance.\n",
        "\n",
        "5x2 cross-validation is a special case of nested cross validation, and refers to a 5 repetition of a 2-fold and is introduced as a way of obtaining not only a good estimate of the generalisation error but also a good estimate of the variance of that error (in order to perform statistical tests).\n",
        "\n",
        "Nested CV is not critically important if your data set is large and without outliers. If your data do have outliers, than cross validation performance may be drastically different depending on what fold/folds these outliers are in. Therefore you repeat CV several times.\n",
        "\n",
        "Sources:\n",
        "*  [scikit-learn's documentation](https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html)\n",
        "*  [machinelearningmastery.com](https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/)\n",
        "*  [analyticsvidhya.com](https://www.analyticsvidhya.com/blog/2021/03/a-step-by-step-guide-to-nested-cross-validation/)\n",
        "*  [stackexchange](https://stats.stackexchange.com/questions/151710/nested-cross-validation-how-is-it-different-from-model-selection-via-kfold-cv)\n",
        "\n",
        "### **Question 17** \n"
      ],
      "metadata": {
        "id": "r6IxiW6v8dLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **Dataset 2**\n",
        "\n",
        "### **Question 1: Bayes Theorem and different classifiers**\n",
        "\n",
        "Bayes' theorem describes the probability of an event, based on prior knowledge of conditions that might be related to that event. Using this theorem, we can use Bayesian inference, which is a statistical inference approach. \n",
        "\n",
        "Bayes' theorem is stated below:\n",
        "\n",
        "$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$\n",
        "\n",
        "Where $A$ and $B$ are events and $P(B)\\neq0$. By calculating the probabilities on the right side of the equation, we can essentially \"work backwards\", and find the probability on the left hand side. \n",
        "\n",
        "For instance, in machine learning, if we were to calculate how likely it was a certain set of features was to be seen given a certain class (e.g. $P(x_1, x_2, x_3 | y)$), over a large enough dataset, we could approximate that given another set of features, such as $x_4, x_5, x_6$, we'd be able to correctly guess $P(y| x_4, x_5, x_6)$.\n",
        "\n",
        "There are different classifiers that work off of this idea that are useful for different scenarios, described below:\n",
        "\n",
        "*  **Gaussian Naive Bayes**: A naive Bayes classifier is called in this way because it’s based on a naive condition, which implies the conditional independence of causes. This can seem very difficult to accept in many contexts where the probability of a particular feature is strictly correlated to another one. Gaussian Naive Bayes is useful when working with continuous values which probabilities can be modeled using a Gaussian distribution.\n",
        "Because of the assumption of the normal distribution, Gaussian Naive Bayes is used in cases when all our features are continuous. For example in Iris dataset features are sepal width, petal width, sepal length, petal length. So its features can have different values in data set as width and length can vary. We can’t represent features in terms of their occurrences. This means data is continuous. Hence we use Gaussian Naive Bayes here.\n",
        "\n",
        "\n",
        "*  **Bernoulli Naive Bayes**: It assumes that all our features are binary such that they take only two values. Their probabilities are as shown below:\n",
        "\n",
        ">  $P(X) = \\left\\{\\begin{matrix}\n",
        " p&  if&  X=1\\\\\n",
        " q&  if&  X=0\\\\\n",
        "\\end{matrix}\\right.\n",
        "where$\n",
        "$q = 1-p$ and $0 < p < 1$\n",
        "\n",
        "*  **Multinomial Naive Bayes**: A multinomial distribution is useful to model feature vectors where each value represents, for example, the number of occurrences of a term or its relative frequency. If the feature vectors have n elements and each of them can assume $k$ different values with probability $p_k$, then:\n",
        "\n",
        ">  $P(X_1 = x_1 \\cap X_2 = x_2 \\cap ... \\cap X_k = x_k) = \\frac{n!}{\\prod_{i}x_i!}\\prod_{i}p_i^{x_i}$\n",
        "\n",
        "\n",
        "\n",
        "Sources:\n",
        "*   [Wikipedia](https://en.wikipedia.org/wiki/Bayes%27_theorem)\n",
        "*   [Quora](https://www.quora.com/What-is-the-difference-between-the-the-Gaussian-Bernoulli-Multinomial-and-the-regular-Naive-Bayes-algorithms)\n",
        "*   [packtpub](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **Question 2**\n",
        "\n",
        "In this, I'll create a Naive Bayes Estimator from scratch using three features: \"chol\", \"trestbps\", and \"thalach\".\n",
        "\n",
        "One issue we have here is that these features are continuous, and not discrete. Here we are faces with many issues, one being the *zero frequency problem* that skews the performance of the entire classifier. It occurs when an instance in a test dataset has a category that was not present during training, and thus will be assigned the probability of 0.\n",
        "\n",
        "All three of our features; `trestbps` (which indicate resting bps), `thalach` (which indicates the maximum heartrate achieved) and `chol` (which represents cholesterol) are continuous, and during testing we may see values we've never seen before.\n",
        "\n",
        "We can solve this problem by binning, and replacing values with thir bin number. Similar to before, we'll use `scikit-learn` to simplify the process of binning. We'll give each feature 10 bins.\n",
        "\n",
        "For ease of calculation, we'll assume all 3 of these features are independent."
      ],
      "metadata": {
        "id": "FUxt75JX9ah-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load second dataset\n",
        "df = pd.read_csv('heart.csv')\n",
        "\n",
        "features_to_use = ['chol', 'trestbps', 'thalach']\n",
        "columns_to_drop = [col for col in list(df.columns) if col not in features_to_use]\n",
        "\n",
        "X_all = df.copy()\n",
        "X_all = X_all.drop(columns=columns_to_drop, axis=1)\n",
        "\n",
        "Y_all = df['target']\n",
        "\n",
        "# Bin continuous features.\n",
        "N_bins = 10\n",
        "est = KBinsDiscretizer(n_bins=N_bins, encode='ordinal', strategy='quantile')\n",
        "for feature in features_to_use:\n",
        "  current_col = X_all[feature].values.reshape(-1, 1)\n",
        "  est.fit(current_col)\n",
        "  X_all[feature] = est.transform(current_col)\n",
        "  df[feature] = X_all[feature]\n",
        "\n",
        "# Train/test split - for the next question\n",
        "X, test_x, Y, test_y = train_test_split(\n",
        "    X_all, Y_all, test_size=0.2, random_state=0\n",
        ")\n",
        "\n",
        "##### Naive Bayes Model\n",
        "# First, calculate probabilities for all classes. (priors)\n",
        "classes = list(Y_all.unique())\n",
        "priors = {}\n",
        "\n",
        "for class_ in classes:\n",
        "  n_class = df[df['target'] == class_].shape[0]\n",
        "  priors[class_] = n_class / Y.shape[0]\n",
        "\n",
        "# Second, calculate a specific feature value likelihood for each target class given a Gaussian distribution \n",
        "likelihoods = {}\n",
        "df_per_class = [df[df['target'] == c] for c in classes]\n",
        "\n",
        "for i, c in enumerate(classes):\n",
        "  current_dict = {}\n",
        "  current_df = df_per_class[i]\n",
        "  for feature in features_to_use:\n",
        "    current_dict[feature] = {}\n",
        "    mean, std = current_df[feature].mean(), current_df[feature].std()\n",
        "    unique_vals_for_feature = list(X_all[feature].unique())\n",
        "    for u_feature in unique_vals_for_feature:\n",
        "      x_given_y = (1 / (np.sqrt(2 * np.pi) * std)) *  np.exp(-((u_feature-mean)**2 / (2 * std**2 )))\n",
        "      current_dict[feature][u_feature] = x_given_y\n",
        "  likelihoods[c] = current_dict.copy()\n",
        "\n",
        "# Final prediction method\n",
        "def calculate_probability(X, likelihoods_dict = likelihoods, class_priors = priors):\n",
        "  results = []\n",
        "  for x in X.values:\n",
        "    probabilities = [1, 1]\n",
        "    for i, c in enumerate(classes):\n",
        "      for feat_idx, feat_val in enumerate(x):\n",
        "        try:\n",
        "          probabilities[i] *= likelihoods_dict[c][features_to_use[feat_idx]][feat_val]\n",
        "        except KeyError:\n",
        "          probabilities[i] = 0\n",
        "          break\n",
        "      probabilities[i] *= class_priors[c]\n",
        "    results.append(probabilities.copy())\n",
        "  final_pred = [np.argmax(res) for res in results]\n",
        "  return final_pred\n",
        "\n",
        "predictions = calculate_probability(test_x)\n",
        "print(f\"Accuracy: {accuracy_score(predictions, test_y)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwCbOuhd9u6I",
        "outputId": "cf78ebd8-34a2-464f-8e5d-a840230ad02e"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.22950819672131148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
            "  \"decreasing the number of bins.\" % jj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 3**\n",
        "\n",
        "In the previous question, I had split the data into a train (`X`/`Y`) and test (`test_x`/`test_y`) set such that 80% of the data was allocated to training and 20% of the data was allocated to testing.\n",
        "\n",
        "For calculating precision, recall and the f1 score, I'll use the same method as I had in the previous section."
      ],
      "metadata": {
        "id": "UJRLWUFre9sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, f1_score, _ = precision_recall_fscore_support(test_y, predictions, beta=1)\n",
        "\n",
        "print(f\"Precision:{precision}, recall:{recall}, f1_score:{f1_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tr8TZn91fwTO",
        "outputId": "f56c7401-6eff-4ad9-e779-7ab563628d54"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision:[0.23684211 0.2173913 ], recall:[0.33333333 0.14705882], f1_score:[0.27692308 0.1754386 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 4**\n",
        "\n",
        "As mentioned in the question, `scikit-learn` provides us with an implementation of Gaussian Naive Bayes known as `GaussianNB` (as documented [here](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)). For ease and higher accuracy, I'll be using the binned data."
      ],
      "metadata": {
        "id": "VlIaoFP1f7sR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "clf = GaussianNB()\n",
        "clf.fit(X, Y)\n",
        "y_pred = clf.predict(test_x)\n",
        "print(f\"Accuracy: {accuracy_score(y_pred, test_y)}\")\n",
        "\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(test_y, y_pred, beta=1)\n",
        "print(f\"Precision:{precision}, recall:{recall}, f1_score:{f1_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvqaN-fbgV2n",
        "outputId": "32f2c0bf-d390-4b51-e23a-bb5286a36a08"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8032786885245902\n",
            "Precision:[0.77777778 0.82352941], recall:[0.77777778 0.82352941], f1_score:[0.77777778 0.82352941]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 5**\n",
        "\n",
        "It's quite clear that `scikit-learn`'s model has performed *much* better than my implementation with `numpy`. This is mostly due to the fact that `GaussianNB` has a few built-in features that allow it to learn more from the data without being so sensitive to fluctutations in the data, such as normal distribution and whatnot. One example of these features can be seen in the `var_smoothing` variable which specifies the portion of the largest variance of all features that is added to variances for calculation stability."
      ],
      "metadata": {
        "id": "rtANfrwng9D3"
      }
    }
  ]
}